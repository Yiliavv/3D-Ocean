# 深度学习调参手册

*这不是 Google 官方支持的产品。*

**Varun Godbole<sup>&dagger;</sup>, George E. Dahl<sup>&dagger;</sup>, Justin Gilmer<sup>&dagger;</sup>, Christopher J. Shallue<sup>&Dagger;</sup>, Zachary Nado<sup>&dagger;</sup>**


&dagger; Google Research, Brain Team

&Dagger; Harvard University

## 目录

-   [本文档适合谁阅读？](#本文档适合谁阅读)
-   [为什么需要调参手册？](#为什么需要调参手册)
-   [新项目起步指南](#新项目起步指南)
    -   [选择模型架构](#选择模型架构)
    -   [选择优化器](#选择优化器)
    -   [选择批次大小](#选择批次大小)
    -   [选择初始配置](#选择初始配置)
-   [改进模型性能的科学方法](#改进模型性能的科学方法)
    -   [增量调参策略](#增量调参策略)
    -   [探索与利用](#探索与利用)
    -   [选择下一轮实验的目标](#选择下一轮实验的目标)
    -   [设计下一轮实验](#设计下一轮实验)
    -   [决定是否采纳训练流程变更或超参数配置](#确定是否采用训练流程更改或超参数配置)
    -   [探索阶段结束后](#探索阶段结束后)
-   [确定每次训练运行的步数](#确定每次训练运行的步数)
    -   [在非计算受限情况下决定训练时长](#在训练不受计算限制时决定训练时长)
    -   [在计算受限情况下决定训练时长](#在训练受计算限制时决定训练时长)
-   [训练流程的附加指导](#训练流程的附加指导)
    -   [优化输入流程](#优化输入流程)
    -   [评估模型性能](#评估模型性能)
    -   [保存检查点并回溯选择最佳检查点](#保存检查点并回溯选择最佳检查点)
    -   [设置实验跟踪](#设置实验跟踪)
    -   [批归一化实现细节](#批归一化实现细节)
    -   [多主机流程的注意事项](#多主机流程的注意事项)
-   [常见问题](#常见问题)
-   [致谢](#致谢)
-   [引用](#引用)
-   [贡献](#贡献)

## 本文档适合谁阅读？

本文档适用于对**最大化深度学习模型性能**感兴趣的工程师和研究人员（个人或团队）。我们假设读者具备机器学习和深度学习的基础知识。

我们的重点是**超参数调优过程**。我们也会涉及深度学习训练的其他方面，如流程实现和优化，但对这些方面的讨论并不全面。

我们假设机器学习问题是监督学习问题，或者与之非常相似（如自监督学习）。也就是说，本文档中的一些建议也可能适用于其他类型的问题。

## 为什么需要调参手册？

目前，要让深度神经网络在实践中真正发挥作用，涉及到令人惊讶的大量工作和猜测。更糟糕的是，人们用来获得良好深度学习结果的实际方法很少被记录下来。论文为了呈现更清晰的叙述，往往会掩盖导致最终结果的过程，而从事商业问题的机器学习工程师很少有时间退后一步概括他们的过程。教科书倾向于回避实践指导，优先考虑基本原理，即使其作者在应用工作中具有提供有用建议所需的经验。在准备创建本文档时，我们找不到任何全面解释*如何通过深度学习获得良好结果*的尝试。相反，我们发现的是博客文章和社交媒体上的零散建议、研究论文附录中透露的技巧、关于某个特定项目或流程的偶尔案例研究，以及大量困惑。深度学习专家取得的成果与使用表面相似方法的技能较低的从业者之间存在巨大鸿沟。与此同时，这些专家自己也坦承，他们所做的一些事情可能并没有充分的理由。随着深度学习的成熟并对世界产生更大的影响，社区需要更多涵盖有用方法的资源，包括所有对获得良好结果至关重要的实践细节。

我们是一个由五名研究人员和工程师组成的团队，在深度学习领域工作多年，其中一些人早在 2006 年就开始了。我们将深度学习应用于从语音识别到天文学等各种问题，并在此过程中学到了很多东西。本文档源于我们自己训练神经网络、教授新机器学习工程师以及为同事提供深度学习实践建议的经验。尽管看到深度学习从少数学术实验室实践的机器学习方法发展成为数十亿人使用的产品背后的技术令人欣慰，但深度学习作为一门工程学科仍处于起步阶段，我们希望本文档能鼓励其他人帮助系统化该领域的实验协议。

本文档是我们试图将自己的深度学习方法具体化的产物，因此它代表了作者在撰写时的观点，而不是任何客观真理。我们自己在超参数调优方面的困难使其成为我们指导的特别关注点，但我们也涵盖了我们在工作中遇到的（或看到出错的）其他重要问题。我们的意图是将这项工作作为一份随着我们的信念变化而增长和演变的动态文档。例如，关于调试和缓解训练失败的材料，我们在两年前是不可能写出来的，因为它基于最近的结果和正在进行的调查。不可避免地，我们的一些建议将需要更新，以说明新结果和改进的工作流程。我们不知道*最优*的深度学习方法，但在社区开始记录和讨论不同程序之前，我们无法希望找到它。为此，我们鼓励发现我们建议存在问题的读者提出替代建议，并提供令人信服的证据，以便我们更新手册。我们也很希望看到可能有不同建议的替代指南和手册，以便我们作为一个社区朝着最佳实践努力。最后，任何标有 🤖 表情符号的部分都是我们希望进行更多研究的地方。只有在尝试编写此手册后，才完全清楚在深度学习从业者的工作流程中可以发现多少有趣和被忽视的研究问题。

## 新项目起步指南

在调参过程中，我们做出的许多决策可以在项目开始时做出一次，只有在情况发生变化时才偶尔重新审视。

我们下面的指导基于以下假设：

-   问题表述、数据清洗等基本工作已经完成足够多，花时间在模型架构和训练配置上是有意义的。
-   已经建立了进行训练和评估的流程，并且可以轻松地为感兴趣的各种模型执行训练和预测作业。
-   已经选择并实施了适当的指标。这些指标应尽可能代表部署环境中将测量的内容。

### 选择模型架构

***摘要：*** *开始新项目时，尽量重用已经有效的模型。*

-   首先选择一个成熟的、常用的模型架构来开始工作。以后总是可以构建自定义模型的。
-   模型架构通常具有各种超参数，这些超参数决定模型的大小和其他细节（例如层数、层宽度、激活函数类型）。
    -   因此，选择架构实际上意味着选择一系列不同的模型（每个模型超参数设置对应一个模型）。
    -   我们将在[选择初始配置](#选择初始配置)和[改进模型性能的科学方法](#改进模型性能的科学方法)中考虑选择模型超参数的问题。
-   如果可能，尝试找到一篇尽可能接近当前问题的论文，并重现该模型作为起点。

### 选择优化器

***摘要：*** *从当前问题类型最流行的优化器开始。*

-   没有哪个优化器在所有类型的机器学习问题和模型架构中都是"最好的"。即使只是[比较优化器的性能也是一项困难的任务](https://arxiv.org/abs/1910.05446)。🤖
-   我们建议坚持使用成熟的、流行的优化器，特别是在开始新项目时。
    -   理想情况下，选择用于相同类型问题的最流行的优化器。
-   准备好关注所选优化器的**所有**超参数。
    -   具有更多超参数的优化器可能需要更多的调优工作才能找到最佳配置。
    -   这在项目的初始阶段尤其相关，当时我们试图找到各种其他超参数（例如架构超参数）的最佳值，同时将优化器超参数视为[干扰参数](#识别科学干扰和固定超参数)。
    -   在项目的初始阶段，最好从更简单的优化器开始（例如具有固定动量的 SGD 或具有固定 $\epsilon$、$\beta_{1}$ 和 $\beta_{2}$ 的 Adam），然后再切换到更通用的优化器。
-   我们喜欢的成熟优化器包括（但不限于）：
    -   [带动量的 SGD](#所有流行优化算法的更新规则是什么)（我们喜欢 Nesterov 变体）
    -   [Adam 和 NAdam](#所有流行优化算法的更新规则是什么)，它们比带动量的 SGD 更通用。请注意，Adam 有 4 个可调超参数，[它们都可能很重要](https://arxiv.org/abs/1910.05446)！
        -   参见[如何调整 Adam 的超参数？](#adam-的超参数应该如何调优)

### 选择批次大小

***摘要：*** *批次大小控制训练速度，不应用于直接调优验证集性能。通常，理想的批次大小将是可用硬件支持的最大批次大小。*

-   批次大小是决定*训练时间*和*计算资源消耗*的关键因素。
-   增加批次大小通常会减少训练时间。这可能非常有益，因为它可以：
    -   允许在固定时间间隔内更彻底地调优超参数，从而可能产生更好的最终模型。
    -   减少开发周期的延迟，允许更频繁地测试新想法。
-   增加批次大小可能会减少、增加或不改变资源消耗。
-   批次大小*不应*被视为用于验证集性能的可调超参数。
    -   只要所有超参数都得到很好的调优（特别是学习率和正则化超参数），并且训练步数足够，使用任何批次大小都应该能够获得相同的最终性能（参见 [Shallue et al. 2018](https://arxiv.org/abs/1811.03600)）。
    -   请参阅[为什么不应调整批次大小以直接改善验证集性能？](#为什么不应该调整批次大小以直接改善验证集性能)

#### 确定可行的批次大小并估计训练吞吐量


<details><summary><em>[点击展开]</em></summary>

<br>

-   对于给定的模型和优化器，通常会有可用硬件支持的批次大小范围。限制因素通常是加速器内存。
-   不幸的是，如果不运行或至少编译完整的训练程序，很难计算哪些批次大小适合内存。
-   最简单的解决方案通常是以不同的批次大小（例如，2 的递增幂）运行训练作业几步，直到其中一个作业超过可用内存。
-   对于每个批次大小，我们应该训练足够长的时间，以获得*训练吞吐量*的可靠估计：

<p align="center">训练吞吐量 = （每秒处理的样本数）</p>

<p align="center">或者等效地，<em>每步时间</em>。</p>

<p align="center">每步时间 = （批次大小）/（训练吞吐量）</p>

-   当加速器尚未饱和时，如果批次大小翻倍，训练吞吐量也应该翻倍（或至少接近翻倍）。等效地，随着批次大小增加，每步时间应该是恒定的（或至少接近恒定）。
-   如果不是这种情况，则训练流程存在瓶颈，例如 I/O 或计算节点之间的同步。在继续之前，诊断和纠正这一点可能是值得的。
-   如果训练吞吐量仅增加到某个最大批次大小，那么我们应该只考虑该最大批次大小以下的批次大小，即使硬件支持更大的批次大小。
    -   使用更大批次大小的所有好处都假设训练吞吐量增加。如果没有增加，请修复瓶颈或使用较小的批次大小。
    -   **梯度累积**模拟比硬件支持的更大的批次大小，因此不提供任何吞吐量好处。在应用工作中通常应避免使用。
-   每次更改模型或优化器时，可能需要重复这些步骤（例如，不同的模型架构可能允许更大的批次大小适合内存）。

</details>

#### 选择批次大小以最小化训练时间

<details><summary><em>[点击展开]</em></summary>

<br>


<p align="center">训练时间 = （每步时间）×（总步数）</p>

-   我们通常可以认为对于所有可行的批次大小，每步时间大致恒定。当没有并行计算的开销并且所有训练瓶颈都已被诊断和纠正时，这是正确的（有关如何识别训练瓶颈，请参阅[上一节](#确定可行的批次大小并估计训练吞吐量)）。在实践中，增加批次大小通常至少会有一些开销。
-   随着批次大小的增加，达到固定性能目标所需的总步数通常会减少（前提是在更改批次大小时重新调整所有相关的超参数；[Shallue et al. 2018](https://arxiv.org/abs/1811.03600)）。
    -   例如，将批次大小翻倍可能会将所需的总步数减半。这称为**完美扩展**。
    -   完美扩展适用于所有批次大小，直到临界批次大小，超过该大小后会出现收益递减。
    -   最终，增加批次大小不再减少训练步数（但也不会增加）。
-   因此，最小化训练时间的批次大小通常是仍然能够减少所需训练步数的最大批次大小。
    -   此批次大小取决于数据集、模型和优化器，除了对每个新问题进行实验性查找外，如何计算它还是一个开放问题。🤖
    -   在比较批次大小时，要注意样本预算/[周期](https://developers.google.com/machine-learning/glossary#epoch)预算（在固定训练样本呈现次数的情况下运行所有实验）和步骤预算（在固定训练步数的情况下运行所有实验）之间的区别。
        -   使用周期预算比较批次大小只能探测完美扩展区域，即使更大的批次大小仍可能通过减少所需的训练步数提供有意义的加速。
    -   通常，可用硬件支持的最大批次大小将小于临界批次大小。因此，一个好的经验法则（不运行任何实验）是使用尽可能大的批次大小。
-   如果使用更大的批次大小最终会增加训练时间，那就没有意义了。

</details>

#### 选择批次大小以最小化资源消耗

<details><summary><em>[点击展开]</em></summary>

<br>


-   增加批次大小会带来两种类型的资源成本：
    1.  *前期成本*，例如购买新硬件或重写训练流程以实现多 GPU / 多 TPU 训练。
    2.  *使用成本*，例如团队资源预算的计费、云提供商的计费、电力/维护成本。
-   如果增加批次大小有显著的前期成本，最好推迟增加批次大小，直到项目成熟并且更容易评估成本效益权衡。实施多主机并行训练程序可能会引入[错误](#多主机流程的注意事项)和[微妙的问题](#批归一化实现细节)，因此最好从更简单的流程开始。（另一方面，当需要大量调优实验时，训练时间的大幅加速可能在过程早期非常有益）。
-   我们将总使用成本（可能包括多种不同类型的成本）称为"资源消耗"。我们可以将资源消耗分解为以下组成部分：

<p align="center">资源消耗 = （每步资源消耗）×（总步数）</p>

-   增加批次大小通常允许我们[减少总步数](#选择批次大小以最小化训练时间)。资源消耗是增加还是减少将取决于每步消耗的变化情况。
    -   增加批次大小可能会*减少*资源消耗。例如，如果具有较大批次大小的每一步都可以在与较小批次大小相同的硬件上运行（每步时间仅略微增加），那么每步资源消耗的任何增加都可能被步数减少所抵消。
    -   增加批次大小可能*不会改变*资源消耗。例如，如果将批次大小翻倍会将所需步数减半并将使用的 GPU 数量翻倍，则总消耗（以 GPU-小时为单位）不会改变。
    -   增加批次大小可能会*增加*资源消耗。例如，如果增加批次大小需要升级硬件，则每步消耗的增加可能超过步数的减少。

</details>

#### 更改批次大小需要重新调整大多数超参数

<details><summary><em>[点击展开]</em></summary>

<br>


-   大多数超参数的最优值对批次大小敏感。因此，更改批次大小通常需要从头开始调优过程。
-   与批次大小交互最强的超参数，因此对于每个批次大小最重要的是分别调优的超参数是优化器超参数（例如学习率、动量）和正则化超参数。
-   在项目开始时选择批次大小时请记住这一点。如果以后需要切换到不同的批次大小，为新批次大小重新调整所有内容可能会很困难、耗时且昂贵。

</details>

#### 批归一化如何与批次大小交互

<details><summary><em>[点击展开]</em></summary>

<br>


-   批归一化很复杂，通常应该使用与梯度计算不同的批次大小来计算统计量。有关详细讨论，请参阅[批归一化部分](#批归一化实现细节)。

</details>

### 选择初始配置

-   在开始超参数调优之前，我们必须确定起点。这包括指定（1）模型配置（例如层数），（2）优化器超参数（例如学习率），以及（3）训练步数。
-   确定此初始配置将需要一些手动配置的训练运行和试错。
-   我们的指导原则是找到一个简单的、相对快速的、相对低资源消耗的配置，以获得"合理"的结果。
    -   "简单"意味着尽可能避免花里胡哨的东西；这些总是可以稍后添加。即使花里胡哨的东西在未来被证明有帮助，在初始配置中添加它们也会冒着浪费时间调优无用功能和/或引入不必要复杂性的风险。
        -   例如，在添加花哨的衰减计划之前，先从恒定学习率开始。
    -   选择快速且消耗最少资源的初始配置将使超参数调优更加高效。
        -   例如，从较小的模型开始。
    -   "合理"的性能取决于问题，但至少意味着训练的模型在验证集上的表现远好于随机猜测（尽管它可能太差而不值得部署）。
-   选择训练步数涉及平衡以下张力：
    -   一方面，训练更多步骤可以提高性能并使超参数调优更容易（参见 [Shallue et al. 2018](https://arxiv.org/abs/1811.03600)）。
    -   另一方面，训练更少步骤意味着每次训练运行更快且使用更少的资源，通过减少周期之间的时间并允许并行运行更多实验来提高调优效率。此外，如果最初选择了不必要的大步骤预算，可能很难在以后更改它，例如一旦为该步数调整了学习率计划。

## 改进模型性能的科学方法

就本文档而言，机器学习开发的最终目标是最大化已部署模型的效用。尽管开发过程的许多方面在不同应用之间有所不同（例如时间长度、可用计算资源、模型类型），但我们通常可以在任何问题上使用相同的基本步骤和原则。

我们下面的指导基于以下假设：

-   已经有一个完全运行的训练流程以及获得合理结果的配置。
-   有足够的计算资源可用于进行有意义的调优实验，并至少并行运行几个训练作业。

### 增量调参策略

***摘要：*** *从简单的配置开始，在建立对问题的洞察的同时逐步改进。确保任何改进都基于强有力的证据，以避免添加不必要的复杂性。*

-   我们的最终目标是找到能够最大化模型性能的配置。
    -   在某些情况下，我们的目标是在固定截止日期前尽可能多地改进模型（例如提交比赛）。
    -   在其他情况下，我们希望无限期地持续改进模型（例如持续改进生产中使用的模型）。
-   原则上，我们可以通过使用算法自动搜索所有可能配置的整个空间来最大化性能，但这不是一个实际的选择。
    -   可能配置的空间极其庞大，还没有足够复杂的算法能够在没有人类指导的情况下有效地搜索这个空间。
-   大多数自动搜索算法依赖于手工设计的*搜索空间*，该空间定义了要搜索的配置集，这些搜索空间可能非常重要。
-   最大化性能的最有效方法是从简单的配置开始，在建立对问题的洞察的同时逐步添加功能并进行改进。
    -   我们在每轮调优中使用自动搜索算法，并随着理解的增长不断更新我们的搜索空间。
-   随着我们的探索，我们自然会找到越来越好的配置，因此我们的"最佳"模型将持续改进。
    -   当我们更新最佳配置时，我们称之为*发布*（这可能对应也可能不对应生产模型的实际发布）。
    -   对于每次发布，我们必须确保更改基于强有力的证据——而不仅仅是基于幸运配置的随机机会——这样我们就不会向训练流程添加不必要的复杂性。

在高层次上，我们的增量调优策略涉及重复以下四个步骤：

1.  为下一轮实验确定一个范围适当的目标。
2.  设计并运行一组朝着此目标前进的实验。
3.  从结果中学习我们能学到的东西。
4.  考虑是否发布新的最佳配置。

本节的其余部分将更详细地考虑这一策略。

### 探索与利用

***摘要：*** *大多数时候，我们的主要目标是获得对问题的洞察。*

-   尽管人们可能认为我们会花费大部分时间尝试最大化验证集上的性能，但实际上我们花费大部分时间试图获得对问题的洞察，而贪婪地关注验证误差的时间相对较少。
    -   换句话说，我们将大部分时间花在"探索"上，只有少量时间用于"利用"。
-   从长远来看，如果我们想最大化最终性能，理解问题至关重要。优先考虑洞察而不是短期收益可以帮助我们：
    -   避免启动不必要的更改，这些更改仅仅是由于历史偶然而出现在表现良好的运行中。
    -   识别验证误差对哪些超参数最敏感，哪些超参数交互最多因此需要一起重新调整，以及哪些超参数对其他更改相对不敏感因此可以在未来实验中固定。
    -   建议尝试潜在的新功能，例如如果过拟合是问题，则建议新的正则化器。
    -   识别没有帮助的功能，因此可以删除，减少未来实验的复杂性。
    -   识别超参数调优的改进何时可能已经饱和。
    -   在最优值周围缩小我们的搜索空间以提高调优效率。
-   当我们最终准备好贪婪时，我们可以纯粹专注于验证误差，即使实验对调优问题的结构不是最有信息量的。

### 选择下一轮实验的目标

***摘要：*** *每轮实验都应该有一个明确的目标，并且范围足够窄，以便实验能够真正朝着目标取得进展。*

-   每轮实验都应该有一个明确的目标，并且范围足够窄，以便实验能够真正朝着目标取得进展：如果我们试图一次添加多个功能或回答多个问题，我们可能无法理清对结果的单独影响。
-   示例目标包括：
    -   尝试对流程进行潜在改进（例如新的正则化器、预处理选择等）。
    -   理解特定模型超参数的影响（例如激活函数）
    -   贪婪地最小化验证误差。

### 设计下一轮实验

***摘要：*** *识别哪些超参数是实验目标的科学超参数、干扰超参数和固定超参数。创建一系列研究来比较科学超参数的不同值，同时优化干扰超参数。选择干扰超参数的搜索空间以平衡资源成本和科学价值。*

#### 识别科学超参数、干扰超参数和固定超参数

<details><summary><em>[点击展开]</em></summary>

<br>

-   对于给定的目标，所有超参数将是**科学超参数**、**干扰超参数**或**固定超参数**。
    -   科学超参数是我们试图测量其对模型性能影响的那些超参数。
    -   干扰超参数是那些需要优化以公平比较科学超参数不同值的超参数。这类似于统计学中的[干扰参数](https://en.wikipedia.org/wiki/Nuisance_parameter)概念。
    -   固定超参数将在当前这轮实验中固定其值。这些是在比较科学超参数的不同值时不需要（或我们不希望）改变其值的超参数。
        -   通过为一组实验固定某些超参数，我们必须接受从实验中得出的结论可能对固定超参数的其他设置无效。换句话说，固定超参数为我们从实验中得出的任何结论创建了注意事项。
-   例如，如果我们的目标是"确定具有更多隐藏层的模型是否会减少验证误差"，那么隐藏层的数量是一个科学超参数。
    -   学习率是一个干扰超参数，因为我们只能在为每个层数单独调整学习率的情况下，公平地比较具有不同隐藏层数的模型（最优学习率通常取决于模型架构）。
    -   如果我们在先前的实验中确定激活函数的最佳选择对模型深度不敏感，或者如果我们愿意将关于隐藏层数量的结论限制为仅涵盖这个特定的激活函数选择，那么激活函数可能是一个固定超参数。或者，如果我们准备为每个隐藏层数单独调整它，它可能是一个干扰参数。
-   特定超参数是科学超参数、干扰超参数还是固定超参数并不是该超参数固有的，而是根据实验目标而变化的。
    -   例如，激活函数的选择可以是科学超参数（ReLU 还是 tanh 对我们的问题是更好的选择？），干扰超参数（当我们允许几种不同的可能激活函数时，最佳的 5 层模型是否优于最佳的 6 层模型？），或固定超参数（对于 ReLU 网络，在特定位置添加批归一化是否有帮助？）。
-   在设计新一轮实验时，我们首先为我们的实验目标识别科学超参数。
    -   在这个阶段，我们将所有其他超参数视为干扰超参数。
-   接下来，我们将一些干扰超参数转换为固定超参数。
    -   如果资源无限，我们会将所有非科学超参数保留为干扰超参数，以便我们从实验中得出的结论不受固定超参数值的注意事项的影响。
    -   然而，我们尝试调优的干扰超参数越多，我们就越有可能无法为科学超参数的每个设置充分调优它们，最终从我们的实验中得出错误的结论。
        -   如[下文](#在信息量大和可负担的实验之间取得平衡)所述，我们可以通过增加计算预算来对抗这种风险，但通常我们的最大资源预算低于调优所有非科学超参数所需的预算。
    -   根据我们的判断，当固定它所引入的注意事项不如将其作为干扰超参数的成本那么繁重时，我们选择将干扰超参数转换为固定超参数。
        -   给定的干扰超参数与科学超参数交互越多，固定其值就越有害。例如，权重衰减强度的最佳值通常取决于模型大小，因此假设权重衰减的单个特定值来比较不同的模型大小不会非常有见地。
-   虽然我们为每个超参数分配的类型取决于实验目标，但对于某些类别的超参数，我们有以下经验法则：
    -   在各种优化器超参数（例如学习率、动量、学习率计划参数、Adam betas 等）中，至少有一些将是干扰超参数，因为它们往往与其他更改交互最多。
        -   它们很少是科学超参数，因为像"当前流程的最佳学习率是什么？"这样的目标不会提供太多洞察——最佳设置可能会随着下一次流程更改而轻易改变。
        -   尽管由于资源限制或当我们有特别强的证据表明它们不与科学参数交互时，我们可能偶尔会固定其中一些，但我们通常应该假设必须单独调优优化器超参数，以便在科学超参数的不同设置之间进行公平比较，因此不应该固定。
            -   此外，我们没有*先验*理由偏好一个优化器超参数值而不是另一个（例如，它们通常不会以任何方式影响前向传播或梯度的计算成本）。
    -   相比之下，优化器的*选择*通常是科学超参数或固定超参数。
        -   如果我们的实验目标涉及在两个或多个不同的优化器之间进行公平比较（例如"确定哪个优化器在给定步数下产生最低的验证误差"），它就是一个科学超参数。
        -   或者，我们可能出于多种原因将其设为固定超参数，包括（1）先前的实验使我们相信我们问题的最佳优化器对当前科学超参数不敏感；和/或（2）我们更喜欢使用此优化器比较科学超参数的值，因为其训练曲线更容易推理；和/或（3）我们更喜欢使用此优化器，因为它比替代方案使用更少的内存。
    -   正则化技术引入的超参数通常是干扰超参数，但我们是否包含正则化技术本身是科学或固定超参数。
        -   例如，dropout 增加了代码复杂性，因此在决定是否包含它时，我们会将"无 dropout"与"dropout"作为科学超参数，将 dropout 率作为干扰超参数。
            -   如果我们根据此实验决定向我们的流程添加 dropout，那么 dropout 率将在未来的实验中成为干扰超参数。
    -   架构超参数通常是科学或固定超参数，因为架构更改会影响服务和训练成本、延迟和内存需求。
        -   例如，层数通常是科学或固定超参数，因为它往往对训练速度和内存使用产生巨大影响。
-   在某些情况下，干扰超参数和固定超参数的集合将取决于科学超参数的值。
    -   例如，假设我们试图确定 Nesterov 动量和 Adam 中哪个优化器产生最低的验证误差。科学超参数是 `optimizer`，它采用值 `{"Nesterov_momentum", "Adam"}`。值 `optimizer="Nesterov_momentum"` 引入了干扰/固定超参数 `{learning_rate, momentum}`，但值 `optimizer="Adam"` 引入了干扰/固定超参数 `{learning_rate, beta1, beta2, epsilon}`。
    -   仅对科学超参数的某些值存在的超参数称为**条件超参数**。
    -   我们不应该仅仅因为两个条件超参数具有相同的名称就假设它们相同！在上面的示例中，对于 `optimizer="Nesterov_momentum"` 与 `optimizer="Adam"`，称为 `learning_rate` 的条件超参数是*不同的*超参数。它在两种算法中的作用相似（尽管不完全相同），但在每个优化器中效果良好的值范围通常相差几个数量级。

</details>

#### 创建一组研究

<details><summary><em>[点击展开]</em></summary>

<br>


-   一旦我们识别了科学超参数和干扰超参数，我们就设计一个"研究"或一系列研究来朝着实验目标前进。
    -   研究指定一组要运行以供后续分析的超参数配置。每个配置称为一个"试验"。
    -   创建研究通常涉及选择将在试验之间变化的超参数、选择这些超参数可以采用的值（"搜索空间"）、选择试验数量，以及选择自动搜索算法从搜索空间中采样那么多试验。或者，我们可以通过手动指定超参数配置集来创建研究。
-   研究的目的是使用科学超参数的不同值运行流程，同时**"优化掉"**（或"优化"）干扰超参数，以便科学超参数不同值之间的比较尽可能公平。
-   在最简单的情况下，我们会为科学参数的每个配置创建一个单独的研究，其中每个研究调优干扰超参数。
    -   例如，如果我们的目标是从 Nesterov 动量和 Adam 中选择最佳优化器，我们可以创建一个研究，其中 `optimizer="Nesterov_momentum"` 且干扰超参数为 `{learning_rate, momentum}`，以及另一个研究，其中 `optimizer="Adam"` 且干扰超参数为 `{learning_rate, beta1, beta2, epsilon}`。我们将通过从每个研究中选择表现最好的试验来比较这两个优化器。
    -   我们可以使用任何无梯度优化算法，包括贝叶斯优化或进化算法等方法，来优化干扰超参数，尽管[我们更喜欢](#为什么在调优的探索阶段使用准随机搜索而不是更复杂的黑盒优化算法)在调优的[探索阶段](#探索与利用)使用准随机搜索，因为它在此设置中具有多种优势。[在探索结束后](#探索阶段结束后)，如果有最先进的贝叶斯优化软件可用，那是我们的首选。
-   在更复杂的情况下，我们想要比较大量科学超参数值，并且制作那么多独立研究是不切实际的，我们可以将科学参数包含在与干扰超参数相同的搜索空间中，并使用搜索算法在单个研究中对*科学超参数和干扰超参数*的值进行采样。
    -   采用这种方法时，条件超参数可能会导致问题，因为除非干扰超参数集对于科学超参数的所有值都相同，否则很难指定搜索空间。
    -   在这种情况下，[我们更喜欢](#为什么在调优的探索阶段使用准随机搜索而不是更复杂的黑盒优化算法)使用准随机搜索而不是更复杂的黑盒优化工具，因为它确保我们获得科学超参数值的相对均匀采样。无论搜索算法如何，我们都需要以某种方式确保它均匀地搜索科学参数。

</details>

#### 在信息性和可负担性实验之间取得平衡

<details><summary><em>[点击展开]</em></summary>

<br>


-   在设计研究或一系列研究时，我们需要分配有限的预算，以充分实现以下三个期望：
    1.  比较足够多的科学超参数的不同值。
    2.  在足够大的搜索空间上调优干扰超参数。
    3.  足够密集地采样干扰超参数的搜索空间。
-   我们越能实现这三个期望，就能从实验中提取越多的洞察。
    -   比较尽可能多的科学超参数值可以拓宽我们从实验中获得的洞察的范围。
    -   包含尽可能多的干扰超参数，并允许每个干扰超参数在尽可能宽的范围内变化，增加我们的信心，即对于科学超参数的每个配置，干扰超参数的"好"值**存在**于搜索空间中。
        -   否则，我们可能会通过不搜索干扰参数空间中可能存在更好值的可能区域（对于某些科学参数值），在科学超参数值之间进行不公平的比较。
    -   尽可能密集地采样干扰超参数的搜索空间，增加我们的信心，即我们搜索空间中恰好存在的干扰超参数的任何好设置都将被搜索过程找到。
        -   否则，我们可能会由于某些值在干扰超参数的采样中更幸运而在科学参数值之间进行不公平的比较。
-   不幸的是，在这三个维度中*任何一个*的改进都需要增加试验数量，从而增加资源成本，或者找到一种在其他维度之一节省资源的方法。
    -   每个问题都有其自己的特性和计算约束，因此如何在这三个期望之间分配资源需要一定程度的领域知识。
    -   运行研究后，我们总是试图了解研究是否充分调优了干扰超参数（即足够广泛地搜索了足够大的空间）以公平地比较科学超参数（如[下文](#从实验结果中提取洞察)更详细描述的那样）。

</details>

### 从实验结果中提取洞察

***摘要：*** *除了尝试实现每组实验的原始科学目标外，还要检查额外问题的清单，如果发现问题，修改实验并重新运行它们。*

-   最终，每组实验都有一个特定的目标，我们想要评估实验为该目标提供的证据。
    -   但是，如果我们提出正确的问题，我们通常会发现在给定的一组实验能够朝着其原始目标取得很大进展之前需要纠正的问题。
        -   如果我们不问这些问题，我们可能会得出错误的结论。
    -   由于运行实验可能很昂贵，我们也想借此机会从每组实验中提取其他有用的洞察，即使这些洞察与当前目标没有直接关系。
-   在分析给定的一组实验以朝着其原始目标取得进展之前，我们应该问自己以下额外问题：
    -   [搜索空间足够大吗？](#识别不良的搜索空间边界)
        -   如果研究的最优点在一个或多个维度上靠近搜索空间的边界，则搜索可能不够宽。在这种情况下，我们应该使用扩展的搜索空间运行另一个研究。
    -   [我们从搜索空间中采样了足够的点吗？](#搜索空间中采样的点不够)
        -   如果没有，运行更多点或在调优目标上不那么雄心勃勃。
    -   每个研究中有多少比例的试验是**不可行的**（即发散的试验、获得非常糟糕的损失值的试验，或因违反某些隐式约束而根本无法运行的试验）？
        -   当研究中非常大比例的点是**不可行的**时，我们应该尝试调整搜索空间以避免采样这样的点，这有时需要重新参数化搜索空间。
        -   在某些情况下，大量不可行点可能表示训练代码中存在错误。
    -   [模型是否表现出优化问题？](#如何调试和缓解优化失败)
    -   [我们可以从最佳试验的训练曲线中学到什么？](#检查训练曲线)
        -   例如，最佳试验是否具有与问题性过拟合一致的训练曲线？
-   如有必要，根据上述问题的答案，改进最近的研究（或一组研究）以改进搜索空间和/或采样更多试验，或采取其他纠正措施。
-   一旦我们回答了上述问题，我们就可以继续评估实验为我们的原始目标提供的证据（例如，[评估更改是否有用](#使用隔离图检测更改是否有用)）。

#### 识别不良的搜索空间边界

<details><summary><em>[点击展开]</em></summary>

<br>


-   如果从搜索空间中采样的最佳点接近其边界，则该搜索空间是可疑的。如果我们在该方向扩展搜索范围，我们可能会找到更好的点。
-   为了检查搜索空间边界，我们喜欢在我们称之为**基本超参数轴图**的图上绘制完成的试验，在那里我们绘制验证目标值与其中一个超参数（例如学习率）的关系。图上的每个点对应一个单独的试验。
    -   每个试验的验证目标值通常应该是它在训练过程中达到的最佳值。

<p align="center" id="figure-1">
    <img src="https://raw.githubusercontent.com/google-research/tuning_playbook/main/assets/bad_search_space.png" width="49%" alt="不良搜索空间边界的示例">
<img src="https://raw.githubusercontent.com/google-research/tuning_playbook/main/assets/good_search_space.png" width="49%" alt="可接受的搜索空间边界的示例">
</p>

<p align="center"><b>图 1：</b> 不良搜索空间边界和可接受的搜索空间边界的示例。</p>

-   [图 1](#figure-1) 中的图显示了相对于初始学习率的错误率（越低越好）。
-   如果最佳点（在某个维度上）聚集在搜索空间的边缘，则可能需要扩展搜索空间边界，直到观察到的最佳点不再接近边界。
-   通常，研究会包括发散或获得非常糟糕结果的"不可行"试验（在上述图中用红色 X 标记）。
    -   如果所有试验对于大于某个阈值的学习率都是不可行的，并且如果表现最好的试验的学习率位于该区域的边缘，则模型[可能存在稳定性问题，阻止其访问更高的学习率](#如何调试和缓解优化失败)。

</details>

#### 搜索空间中采样的点不够

<details><summary><em>[点击展开]</em></summary>

<br>


-   一般来说，[很难知道](#使用准随机搜索需要多少次试验才能获得良好结果)搜索空间是否已被足够密集地采样。🤖
-   运行更多试验当然更好，但伴随着明显的成本。
-   由于很难知道我们何时已经采样足够，我们通常采样我们能够负担得起的，并尝试通过反复查看各种超参数轴图并试图了解搜索空间的"良好"区域中有多少点来校准我们的直觉信心。

</details>

#### 检查训练曲线

<details><summary><em>[点击展开]</em></summary>

<br>


***摘要：*** *检查训练曲线是识别常见失败模式的简单方法，可以帮助我们确定接下来要采取的行动的优先级。*

-   尽管在许多情况下，我们实验的主要目标只需要考虑每个试验的验证误差，但我们在将每个试验简化为单个数字时必须小心，因为它可能隐藏关于底层发生情况的重要细节。
-   对于每个研究，我们总是查看至少最佳几个试验的**训练曲线**（训练误差和验证误差相对于训练步骤在训练持续时间内的绘制）。
-   即使这对于实现主要实验目标不是必需的，检查训练曲线也是识别常见失败模式的简单方法，可以帮助我们确定接下来要采取的行动的优先级。
-   在检查训练曲线时，我们对以下问题感兴趣。
-   是否有任何试验表现出**问题性过拟合？**
    -   问题性过拟合发生在验证误差在训练过程中的某个时刻开始*增加*时。
    -   在通过为科学超参数的每个设置选择"最佳"试验来优化掉干扰超参数的实验设置中，我们应该*至少*检查与我们正在比较的科学超参数设置对应的每个最佳试验的问题性过拟合。
        -   如果任何最佳试验表现出问题性过拟合，我们通常希望在比较科学超参数的值之前，使用额外的正则化技术重新运行实验和/或更好地调整现有正则化参数。
            -   如果科学超参数包括正则化参数，则可能不适用，因为那样的话，如果这些正则化参数的低强度设置导致问题性过拟合就不足为奇了。
        -   使用增加最小代码复杂性或额外计算的常见正则化技术（例如 dropout、标签平滑、权重衰减），减少过拟合通常很简单，因此在下一轮实验中添加其中一个或多个通常不是大问题。
        -   例如，如果科学超参数是"隐藏层数"，并且使用最大隐藏层数的最佳试验表现出问题性过拟合，那么我们通常更愿意使用额外的正则化再次尝试它，而不是立即选择较小的隐藏层数。
        -   即使"最佳"试验都没有表现出问题性过拟合，如果它发生在*任何*试验中，仍然可能存在问题。
            -   选择最佳试验会抑制表现出问题性过拟合的配置，并偏爱不表现出问题性过拟合的配置。换句话说，它会偏爱具有更多正则化的配置。
            -   然而，任何使训练变得更糟的东西都可以充当正则化器，即使它不是那样设计的。例如，选择较小的学习率可以通过阻碍优化过程来正则化训练，但我们通常不想以这种方式选择学习率。
            -   因此，我们必须意识到，科学超参数的每个设置的"最佳"试验可能以偏好某些科学或干扰超参数的"不良"值的方式被选择。
-   训练后期训练误差或验证误差是否存在高步骤间方差？
    -   如果是这样，这可能会干扰我们比较科学超参数不同值的能力（因为每个试验随机结束在"幸运"或"不幸"的步骤上），以及我们在生产中重现最佳试验结果的能力（因为生产模型可能不会在与研究中相同的"幸运"步骤上结束）。
    -   步骤间方差的最可能原因是批次方差（从训练集中为每个批次随机采样示例）、小验证集，以及在训练后期使用过高的学习率。
    -   可能的补救措施包括增加批次大小、获得更多验证数据、使用学习率衰减或使用 Polyak 平均。
-   试验是否在训练结束时仍在改进？
    -   如果是这样，这表明我们处于["计算受限"区域](#确定每次训练运行的步数)，我们可能会从[增加训练步数](#在训练受计算限制时决定训练时长)或更改学习率计划中受益。
-   训练集和验证集上的性能是否在最终训练步骤之前很久就饱和了？
    -   如果是这样，这表明我们处于["非计算受限"](#确定每次训练运行的步数)区域，我们可能能够[减少训练步数](#在训练不受计算限制时决定训练时长)。
-   尽管我们无法列举所有情况，但从检查训练曲线中可以明显看出许多其他额外的行为（例如，训练过程中训练损失*增加*通常表示训练流程中存在错误）。

</details>

#### 使用隔离图检测更改是否有用

<details><summary><em>[点击展开]</em></summary>

<br>


<p align="center" id="figure-2">
<img src="https://raw.githubusercontent.com/google-research/tuning_playbook/main/assets/isolation_plot.png" width="49%" alt="调查在 ImageNet 上训练的 ResNet-50 的权重衰减最佳值的隔离图。">
</p>

<p align="center"><b>图 2：</b> 调查在 ImageNet 上训练的 ResNet-50 的权重衰减最佳值的隔离图。</p>

-   通常，一组实验的目标是比较科学超参数的不同值。
    -   例如，我们可能想要确定产生最佳验证误差的权重衰减值。
-   **隔离图**是基本超参数轴图的特例。隔离图上的每个点对应于在一些（或所有）干扰超参数上的*最佳*试验的性能。
    -   换句话说，我们在"优化掉"干扰超参数后绘制模型性能。
-   隔离图使得在科学超参数的不同值之间进行同类比较变得更容易。
-   例如，[图 2](#figure-2) 揭示了为在 ImageNet 上训练的 ResNet-50 的特定配置产生最佳验证性能的权重衰减值。
    -   如果我们的目标是确定是否包含权重衰减，那么我们将此图中的最佳点与无权重衰减的基线进行比较。为了公平比较，基线也应该同样很好地调整其学习率。
-   当我们有由（准）随机搜索生成的数据并正在考虑隔离图的连续超参数时，我们可以通过对基本超参数轴图的 x 轴值进行分桶，并在桶定义的每个垂直切片中选择最佳试验来近似隔离图。

</details>

#### 自动生成通用有用的图

<details><summary><em>[点击展开]</em></summary>

<br>

-   生成图所需的工作越多，我们就越不可能像应该的那样经常查看它们，因此我们应该设置我们的基础设施以自动生成尽可能多的图。
-   至少，我们自动为实验中变化的所有超参数生成基本超参数轴图。
-   此外，我们自动为所有试验生成训练曲线，并使查找每个研究的最佳几个试验并检查它们的训练曲线尽可能容易。
-   我们可以添加许多其他可能有用的潜在图和可视化。尽管上面描述的那些是一个好的起点，但套用 Geoffrey Hinton 的话，"每次你绘制新的东西，你就会学到新的东西。"

</details>

### 确定是否采用训练流程更改或超参数配置

***摘要：*** *在决定是否对我们的模型或训练程序进行更改或采用新的超参数配置时，我们需要意识到结果中的不同变异来源。*

-   当我们试图改进我们的模型时，我们可能会观察到与现有配置相比，特定的候选更改最初实现了更好的验证误差，但在重复实验后发现没有一致的优势。非正式地，我们可以将可能导致这种不一致结果的最重要的变异来源分组为以下广泛的类别：
    -   **训练程序方差**、**重训练方差**或**试验方差**：我们在使用相同超参数但不同随机种子的训练运行之间看到的变异。
        -   例如，不同的随机初始化、训练数据洗牌、dropout 掩码、数据增强操作的模式和并行算术操作的顺序都是试验方差的潜在来源。
    -   **超参数搜索方差**或**研究方差**：由我们选择超参数的程序引起的结果变异。
        -   例如，我们可能使用特定的搜索空间运行相同的实验，但使用两个不同的准随机搜索种子，最终选择不同的超参数值。
    -   **数据收集和采样方差**：来自任何类型的训练、验证和测试数据的随机分割的方差，或更广泛地由于训练数据生成过程产生的方差。
-   使用严格的统计测试对在有限验证集上估计的验证错误率进行比较是很好的，但通常仅试验方差就可以在使用相同超参数设置的两个不同训练模型之间产生统计显著差异。
-   当试图得出超出超参数空间中单个点级别的结论时，我们最关心研究方差。
    -   研究方差取决于试验数量和搜索空间，我们看到过它比试验方差大的情况，也看到过它小得多的情况。
-   因此，在采用候选更改之前，考虑运行最佳试验 N 次以表征运行间试验方差。
    -   通常，我们只能在流程发生重大更改后重新表征试验方差，但在某些应用中，我们可能需要更新的估计。
    -   在其他应用中，表征试验方差的成本太高而不值得。
-   归根结底，尽管我们只想采用产生真正改进的更改（包括新的超参数配置），但要求对某些东西有帮助的完全确定性也不是正确的答案。
-   因此，如果新的超参数点（或其他更改）获得比基线更好的结果（尽可能考虑新点和基线的重训练方差），那么我们可能应该将其采用为未来比较的新基线。
    -   但是，我们应该只采用产生的改进超过它们添加的任何复杂性的更改。

### 探索阶段结束后

***摘要：*** *一旦我们完成了对良好搜索空间的探索并决定了应该调整哪些超参数，贝叶斯优化工具是一个引人注目的选择。*

-   在某个时刻，我们的优先事项将从更多地了解调优问题转向生成单个最佳配置以发布或以其他方式使用。
-   此时，应该有一个精炼的搜索空间，它舒适地包含观察到的最佳试验周围的局部区域，并且已经被充分采样。
-   我们的探索工作应该已经揭示了最重要的超参数（以及它们的合理范围），我们可以使用这些超参数来构建使用尽可能大的调优预算的最终自动调优研究的搜索空间。
-   由于我们不再关心最大化我们对调优问题的洞察，[准随机搜索的许多优势](#为什么在调优的探索阶段使用准随机搜索而不是更复杂的黑盒优化算法)不再适用，应该使用贝叶斯优化工具来自动找到最佳超参数配置。
    -   [Open-Source Vizier](https://github.com/google/vizier) 实现了用于调优 ML 模型的各种复杂算法，包括贝叶斯优化算法。
    -   如果搜索空间包含非平凡体积的发散点（获得 NaN 训练损失的点，甚至训练损失比平均值差许多标准偏差的点），则重要的是使用正确处理发散试验的黑盒优化工具（有关处理此问题的极好方法，请参见 [Bayesian Optimization with Unknown Constraints](https://arxiv.org/abs/1403.5607)）。[Open-Source Vizier](https://github.com/google/vizier) 通过将试验标记为不可行来支持发散点，尽管根据其配置方式，它可能不使用我们来自 [Gelbart et al.](https://arxiv.org/abs/1403.5607) 的首选方法。
-   此时，我们还应该考虑检查测试集上的性能。
    -   原则上，我们甚至可以将验证集折叠到训练集中，并使用贝叶斯优化找到的最佳配置重新训练。但是，这仅在不会有针对此特定工作负载的未来发布时才合适（例如一次性 Kaggle 比赛）。

## 确定每次训练运行的步数

-   有两种类型的工作负载：受计算限制的和不受计算限制的。
-   当训练是**计算受限**时，训练受我们愿意等待多长时间的限制，而不受我们拥有多少训练数据或其他因素的限制。
    -   在这种情况下，如果我们能够以某种方式训练更长时间或更有效地训练，我们应该看到更低的训练损失，并且通过适当的调优，改进的验证损失。
    -   换句话说，*加速*训练等同于*改进*训练，"最佳"训练时间始终是"我们能负担得起的时间"。
    -   也就是说，仅仅因为工作负载是计算受限的并不意味着训练更长时间/更快是改进结果的唯一方法。
-   当训练**不受计算限制**时，我们可以训练我们想要的时间，并且在某个时刻，训练更长时间并没有太大帮助（甚至会导致问题性过拟合）。
    -   在这种情况下，我们应该期望能够训练到非常低的训练损失，以至于训练更长时间可能会略微降低训练损失，但不会有意义地降低验证损失。
    -   特别是当训练不受计算限制时，更宽松的训练时间预算可以使调优更容易，尤其是在调优学习率衰减计划时，因为它们与训练预算有特别强的相互作用。
        -   换句话说，非常吝啬的训练时间预算可能需要完美调优学习率衰减计划才能实现良好的错误率。
-   无论给定的工作负载是否受计算限制，增加梯度方差（跨批次）的方法通常会导致训练进度变慢，因此可能会增加达到特定验证损失所需的训练步数。高梯度方差可能由以下原因引起：
    -   使用较小的批次大小
    -   添加数据增强
    -   添加某些类型的正则化（例如 dropout）

### 在训练不受计算限制时决定训练时长

-   我们的主要目标是确保我们训练足够长的时间以使模型达到最佳可能结果，同时避免在训练步数上过度浪费。
-   如有疑问，宁可训练更长时间。假设正确使用回溯性（最优）检查点选择并且检查点足够频繁，训练更长时间时性能永远不应该下降。
-   永远不要在研究中调整 `max_train_steps` 数字。选择一个值并将其用于所有试验。从这些试验中，绘制回溯性检查点选择找到的训练步骤，以优化 `max_train_steps` 的选择。
    -   例如，如果最佳步骤总是在训练的前 10% 期间，那么最大步数太高了。
    -   或者，如果最佳步骤始终在训练的最后 25%，我们可能会从训练更长时间和重新调整衰减计划中受益。
-   当架构或数据发生变化时（例如添加数据增强），理想的训练步数可能会发生变化。
-   下面我们描述如何根据使用恒定学习率"完美拟合"训练集所需的步数来选择 `max_train_steps` 的初始候选值。
    -   请注意，我们不是以精确或数学上明确定义的方式使用短语"完美拟合训练集"。它仅仅是一个非正式的描述符，用于表示非常低的训练损失。
        -   例如，当使用对数损失进行训练时，在没有正则化项的情况下，我们可能会看到训练损失持续缓慢改善，直到我们达到浮点限制，因为网络权重无限制地增长，模型对训练集的预测变得越来越自信。在这种情况下，我们可能会说模型在训练集上的错误分类错误达到零时"完美拟合"了训练集。
    -   如果训练程序中的梯度噪声量增加，我们找到的 `max_train_steps` 的起始值可能需要增加。
        -   例如，如果向模型引入数据增强或像 dropout 这样的正则化器。
    -   如果训练过程以某种方式改善，可能可以减少 `max_train_steps`。
        -   例如，使用更好调优的优化器或更好调优的学习率计划。

#### 使用学习率扫描选择 max_train_steps 初始候选值的算法

<details><summary><em>[点击展开]</em></summary>

<br>

-   此过程假设不仅可以"完美"拟合训练集，而且可以使用恒定学习率计划来实现。
-   如果可以完美拟合整个训练集，那么必须存在一个配置（具有某个 `max_train_steps` 值）可以完美拟合训练集；找到任何这样的配置并使用其 `max_train_steps` 值作为起点 `N`。
-   运行恒定学习率扫描（即网格搜索学习率），不使用数据增强且不使用正则化，其中每个试验训练 `N` 步。
-   扫描中最快的试验达到完美训练性能所需的步数是我们对 `max_train_steps` 的初始猜测。
-   **注意：**糟糕的搜索空间可能会让人自欺欺人。
    -   例如，如果研究中的所有学习率都太小，我们可能会错误地得出结论，认为需要非常大的 `max_train_steps` 值。
    -   至少，我们应该检查研究中的最佳学习率是否不在搜索空间的边界上。

</details>

### 在训练受计算限制时决定训练时长

-   在某些情况下，训练损失持续无限期地改善，我们的耐心和计算资源成为限制因素。
-   如果训练损失（甚至验证损失）持续无限期地改善，我们应该总是训练我们能负担得起的时间吗？不一定。
    -   我们可能能够通过运行更多更短的实验来更有效地调优，并为我们希望发布的模型保留最长的"生产长度"运行。
    -   随着试验的训练时间接近我们的耐心极限，调优实验变得与我们的潜在发布候选者更相关，但我们可以完成的更少。
    -   可能有许多问题我们可以在仅训练生产长度的 ~10% 时回答，但总是存在一个风险，即我们在此时间限制下的结论不适用于生产长度的 20% 的实验，更不用说 100% 了。
-   在多轮中调优，每轮增加每次试验的训练步数限制是一种明智的方法。
    -   我们可以进行任意多轮，但通常 1-3 轮是最实用的。
    -   本质上，尝试使用周转时间非常快的试验尽可能多地了解问题，在调优彻底性与与最终、最长运行的相关性之间进行权衡。
    -   一旦给定的每次试验时间限制产生了有用的见解，我们可以增加训练时间并继续调优，根据需要双重检查我们从较短运行中得出的结论。
-   作为起点，我们建议进行两轮调优：
    -   第 1 轮：较短的运行以找到良好的模型和优化器超参数。
    -   第 2 轮：在良好的超参数点上进行极少数长时间运行以获得最终模型。
-   从 `第 i 轮` → `第 i+1 轮` 的最大问题是如何调整学习率衰减计划。
    -   在轮次之间调整学习率计划时的一个常见陷阱是使用所有额外的训练步骤而学习率太小。

#### 第 1 轮

<details><summary><em>[点击展开]</em></summary>

<br>

-   不幸的是，无法保证在短期、不完整的训练中找到的良好超参数在训练长度显著增加时仍然是好的选择。但是，对于某些类型的超参数，它们通常有足够的相关性，使第 1 轮有用。
-   我们期望在较短运行中找到的哪些超参数值会转移到较长的训练运行中？对于所有这些，我们需要更多的研究。但根据我们目前所知，以下是作者按转移概率递减顺序的猜测：
    -   很可能转移
        -   早期训练不稳定性可以在第一轮调优中使用较少的训练步数来解决。也许这些超参数是我们拥有的最接近确定转移的东西。
            -   预热长度
            -   初始化
    -   可能转移
        -   模型架构 - 模型架构的显著胜利通常会转移，但可能有许多反例。
    -   可能转移
        -   优化算法/优化器超参数 - 我们认为这会"宽松地"转移。它肯定比上面的东西弱。
        -   数据增强
        -   正则化
            -   如果无法完美拟合训练集，模型可能处于正则化不太可能有很大帮助的状态。
    -   不太可能转移
        -   学习率计划：不太可能完美转移。
            -   [这篇论文](https://arxiv.org/abs/2203.15556) 表明即使衰减计划也会转移，但我们不相信这在一般情况下是正确的。示例：在少量训练步数上调优平方根衰减然后扩展到大量步数将导致大部分训练发生在过小的步数上。
                -   在极端训练预算的限制下，人们可能可以用大多数计划做到"足够好"，但如果进行调优，可能会看到明显的性能改进。
            -   [Understanding Short-Horizon Bias in Stochastic Meta-Optimization](https://arxiv.org/abs/1803.02021) 描述了尝试短视地选择学习率的危险。

</details>

#### 第 2 轮

<details><summary><em>[点击展开]</em></summary>

<br>

-   运行第 1 轮的最佳超参数配置。
-   **（推测）** 🤖 使用额外的步骤延长在高学习率下训练的时期。
    -   例如，如果是线性计划，则保持第 1 轮的衰减长度固定，并延长开始时的恒定学习率时期。
    -   对于余弦衰减，只需保留第 1 轮的基础学习率并扩展 `max_train_steps`，如 [Chinchilla 论文](https://arxiv.org/abs/2203.15556) 中所述。
-   对于具有非常成熟的建模和调优流程以及非常长且昂贵的生产训练运行的团队，更多轮可能是有意义的，但它们通常会过度。
    -   我们已经描述了如何从第 1 步 → 第 2 步转移。如果我们不关心分析时间，如果高效使用计算是首要关注点，那么理想情况下将在许多不同的调优轮次中指数级增加训练运行的长度（以及完成研究的端到端时间）。
        -   在每一轮中，我们系统地确保我们的选择继续站得住脚。
        -   新想法经过一个流程，使用从第 i 步到第 i+1 步越来越长时间运行的实验逐步降低风险。

</details>

## 训练流程的附加指导

### 优化输入流程

***摘要：*** *输入受限流程的原因和干预措施高度依赖于任务；使用分析器并注意常见问题。*

-   使用适当的分析器来诊断输入受限流程。例如，JAX 的 [Perfetto](https://jax.readthedocs.io/en/latest/profiling.html) 或 TensorFlow 的 [TensorFlow profiler](https://www.tensorflow.org/guide/profiler)。
-   最终，具体的原因和干预措施将高度依赖于任务。更广泛的工程考虑（例如最小化磁盘占用）可能会导致更糟糕的输入流程性能。
-   常见原因：
    -   数据与训练过程不在同一位置，导致 I/O 延迟（当通过网络读取训练数据时可能会发生这种情况）。
    -   昂贵的在线数据预处理（考虑离线执行一次并保存）。
    -   干扰数据流程预取的无意同步屏障。例如，在 CommonLoopUtils 中同步设备和主机之间的指标时（[链接](https://github.com/google/CommonLoopUtils/blob/fea2518ada8814a78e1492023fd9f00edb0b0568/clu/metrics.py#L291)）。
-   常见技巧：
    -   检测输入流程以预取示例（例如 [tf.data.Dataset.prefetch](https://www.tensorflow.org/guide/data_performance#prefetching)）
    -   尽早从流程中删除未使用的特征/元数据。
    -   增加为输入流程生成示例的作业数量的复制。例如，通过使用 [tf.data service](https://www.tensorflow.org/api_docs/python/tf/data/experimental/service)。

### 评估模型性能

***摘要：*** *以比训练更大的批次大小运行评估。以规则的步骤间隔运行评估，而不是规则的时间间隔。*

#### 评估设置

<details><summary><em>[点击展开]</em></summary>

<br>

-   我们可以在几种设置中评估模型的性能。
    -   **在线评估** - 当模型在生产环境中提供预测时收集指标。
    -   **离线评估** - 当模型在代表生产环境的离线训练/验证/测试集上运行时收集指标。
    -   **周期性评估** - 在模型训练期间收集指标，这些指标可能是离线评估的代理，和/或在离线评估中使用的数据子集上。
-   在线评估是黄金标准，但在模型开发阶段通常不切实际。
-   根据问题，离线评估可能相当复杂且计算成本高昂。
-   周期性评估是最实用和经济的选择，但可能不完全代表生产环境。
    -   我们在周期性评估期间的目标是使用离线评估的便捷代理，而不牺牲我们在训练期间获得的信号的可靠性。

</details>

#### 设置周期性评估

<details><summary><em>[点击展开]</em></summary>

<br>

-   我们在训练期间运行周期性评估以实时监控其进度，以[促进回溯性模型检查点选择](#保存检查点并回溯选择最佳检查点)，以便我们可以[在训练结束时检查训练曲线](#检查训练曲线)。
-   最简单的配置是在同一计算实例中执行训练和周期性评估，定期在训练和评估之间交替。
    -   在这种情况下，用于执行评估的批次大小应该*至少*与用于训练的批次大小一样大，因为在评估期间不需要维护模型激活，从而降低每个示例的计算要求。
-   周期性评估应该以规则的步骤间隔进行，而不是时间间隔。
    -   基于时间间隔的评估可能会使解释训练曲线变得更困难，特别是当训练可能遭受训练作业的抢占、网络延迟问题等时。
-   验证/测试指标的周期性（当使用打乱的训练/验证/测试分割时）可能表示实现错误，例如测试数据与训练数据重叠，或训练数据未正确打乱。以规则的步骤间隔进行评估可以使这些问题更容易捕获。
-   当评估集不能被批次大小整除时，可能会出现部分批次。确保填充的示例被正确加权，以防止损失函数被它们偏向。通常，这些填充的示例可以被赋予零权重。
-   保存每次评估的足够信息以支持离线分析。理想情况下，我们会保存对选定的单个示例的预测，因为它们对于调试非常有价值。
    -   生成像 [SavedModels](https://www.tensorflow.org/guide/saved_model) 这样的工件可以在评估作业完成后轻松进行临时模型检查。

</details>

#### 为周期性评估选择样本

<details><summary><em>[点击展开]</em></summary>

<br>

-   周期性评估作业可能运行得不够快，无法在合理的时间内计算完整离线评估集上的指标。这通常需要为周期性评估采样数据。
-   在构建采样数据集时，我们考虑以下因素：
    -   <ins>样本大小</ins>
        -   检查周期性作业使用的采样数据集上计算的性能是否与整个离线评估集上的性能相匹配，即采样集和完整数据集之间没有偏差。
        -   用于周期性评估的数据集应该足够小，以便于在其整体上生成模型预测，但足够大，以便可以准确测量模型的改进（即不会被标签噪声淹没）。
        -   它应该足够大，以容纳跨试验的多个这样的顺序评估，并仍然产生准确的估计。也就是说，避免随着时间的推移自适应地"拟合"到验证集，以一种不能推广到保留的测试集的方式。但是，这种考虑很少是实际问题。
    -   <ins>不平衡数据集</ins>
        -   对于不平衡数据集，稀有类别示例的性能通常会很嘈杂。
        -   对于类标签中示例数量少的数据集，记录正确预测的示例数量，以获得对准确性改进的更多见解（0.05 的敏感性改进听起来令人兴奋，但它只是多了一个正确的示例吗？）。

</details>

### 保存检查点并回溯选择最佳检查点

***摘要：*** *运行固定步数的训练，并从运行中回溯选择最佳检查点。*

-   大多数深度学习框架都支持[模型检查点](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html)。也就是说，模型的当前状态会定期保存到磁盘上。这使得训练作业能够对计算实例中断具有弹性。
-   最佳检查点通常不是最后一个检查点，特别是当验证集性能不会随着时间的推移继续增加，而是围绕特定值波动时。
-   设置流程以跟踪训练期间到目前为止看到的 N 个最佳检查点。在训练结束时，模型选择就是选择训练期间看到的最佳检查点的问题。我们称之为**回溯性最优检查点选择**。
-   通常不需要支持前瞻性早期停止，因为我们预先指定了试验预算，并且正在保留到目前为止看到的 N 个最佳检查点。

### 设置实验跟踪

***摘要：*** *在跟踪不同的实验时，确保记录一些基本信息，如研究中检查点的最佳性能和研究的简短描述。*

-   我们发现，在电子表格中跟踪实验结果对我们处理过的各种建模问题很有帮助。它通常具有以下列：
    -   研究名称
    -   指向研究配置存储位置的链接。
    -   笔记或研究的简短描述。
    -   运行的试验数量
    -   研究中最佳检查点在验证集上的性能。
    -   启动训练所需的特定复现命令或未提交更改的笔记。
-   找到一个至少捕获上面列出的信息并且对执行它的人来说方便的跟踪系统。未跟踪的实验可能根本不存在。

### 批归一化实现细节

***摘要：*** *如今，批归一化通常可以用 LayerNorm 替换，但在无法替换的情况下，在更改批次大小或主机数量时存在棘手的细节。*

-   批归一化使用当前批次上的均值和方差来归一化激活，但在多设备设置中，这些统计信息在每个设备上是不同的，除非显式同步。
-   传闻报告（主要在 ImageNet 上）说，实际上使用仅 ~64 个示例计算这些归一化统计信息在实践中效果更好（参见 [这篇论文](https://arxiv.org/abs/1705.08741) 中的 Ghost Batch Norm）。
-   将总批次大小和用于计算批归一化统计信息的示例数量解耦对于批次大小比较特别有用。
-   Ghost 批归一化实现并不总是正确处理每设备批次大小 > 虚拟批次大小的情况。在这种情况下，我们实际上需要对每个设备上的批次进行子采样，以获得适当数量的批归一化统计示例。
-   在测试模式批归一化中使用的指数移动平均只是训练统计信息的线性组合，因此这些 EMA 只需要在将它们保存到检查点之前同步。但是，一些常见的批归一化实现不同步这些 EMA，只保存第一个设备的 EMA。

### 多主机流程的注意事项

***摘要：*** *对于日志记录、评估、RNG、检查点和数据分片，多主机训练很容易引入错误！*

-   确保流程仅在一个主机上记录日志和检查点。
-   确保在运行评估或检查点之前，批归一化统计信息在主机之间同步。
-   在主机之间具有相同的 RNG 种子（用于模型初始化）以及在主机之间具有不同的种子（用于数据洗牌/预处理）至关重要，因此请确保适当标记它们。
-   通常建议在主机之间分片数据文件以提高性能。

## 常见问题

### 最佳学习率衰减计划家族是什么？

<details><summary><em>[点击展开]</em></summary>

<br>

-   这是一个开放问题。不清楚如何构建一组严格的实验来自信地回答什么是"最佳"学习率衰减计划。
-   尽管我们不知道最佳计划家族，但我们确信拥有某种（非恒定）计划很重要，并且调优它很重要。
-   在优化过程的不同时间，不同的学习率效果最好。拥有某种计划使模型更有可能达到良好的学习率。

</details>

### 我应该使用哪种学习率衰减作为默认值？

<details><summary><em>[点击展开]</em></summary>
<br>

-   我们的偏好是线性衰减或余弦衰减，还有很多其他计划家族可能也很好。

</details>

### 为什么一些论文有复杂的学习率计划？

<details><summary><em>[点击展开]</em></summary>
<br>

-   看到具有复杂分段学习率（LR）衰减计划的论文并不罕见。
-   读者经常想知道作者是如何得出这样复杂的计划的。
-   许多复杂的学习率衰减计划是以临时方式将计划作为验证集性能的函数进行调优的结果：
    1.  以一些简单的学习率衰减（或恒定学习率）开始单次训练运行。
    2.  继续训练直到性能似乎停滞。如果发生这种情况，暂停训练。从这一点以可能更陡峭的学习率衰减计划（或更小的恒定学习率）恢复它。重复此过程直到会议/发布截止日期。
-   轻率地复制产生的*计划*通常不是一个好主意，因为最佳的特定计划将对许多其他超参数选择敏感。
    -   更好的是复制产生计划的*算法*，尽管当任意人类判断产生计划时，这很少可能。
-   如果可以完全自动化，则使用这种对验证误差敏感的计划是可以的，但作为验证误差函数的人工参与计划很脆弱且不易重现，因此我们建议避免它们。
    -   在发布使用此类计划的结果之前，请尝试使其完全可重现。

</details>

### Adam 的超参数应该如何调优？

<details><summary><em>[点击展开]</em></summary>
<br>

-   如上所述，对搜索空间以及应该从搜索空间中采样多少点做出一般性陈述非常困难。请注意，Adam 中并非所有超参数都同等重要。以下经验法则对应于研究中试验数量的不同"预算"。
    -   如果研究中 < 10 次试验，只调整（基础）学习率。
    -   如果 10-25 次试验，调整学习率和 $\beta_1$。
    -   如果 25+ 次试验，调整学习率、$\beta_1$ 和 $\epsilon$。
    -   如果可以运行远超过 25 次试验，还要调整 $\beta_2$。

</details>

### 为什么在调优的探索阶段使用准随机搜索而不是更复杂的黑盒优化算法？

<details><summary><em>[点击展开]</em></summary>

-   准随机搜索（基于[低差异序列](https://en.wikipedia.org/wiki/Low-discrepancy_sequence)）是我们在作为旨在最大化对调优问题的洞察的迭代调优过程的一部分时，相对于更复杂的黑盒优化工具的偏好（我们称之为"探索阶段"）。贝叶斯优化和类似工具更适合利用阶段。
-   基于随机移位低差异序列的准随机搜索可以被认为是"抖动的、打乱的网格搜索"，因为它以统一但随机的方式探索给定的搜索空间，并且比随机搜索更能分散搜索点。
-   准随机搜索相对于更复杂的黑盒优化工具（例如贝叶斯优化、进化算法）的优势包括：
    1.  非自适应地采样搜索空间使得在事后分析中更改调优目标而无需重新运行实验成为可能。
        -   例如，我们通常想要根据训练中任何时刻达到的验证误差找到最佳试验。但准随机搜索的非自适应性质使得可以基于最终验证误差、训练误差或某些替代评估指标找到最佳试验，而无需重新运行任何实验。
    2.  准随机搜索以一致且统计上可重现的方式行为。
        -   即使搜索算法的实现发生变化，只要它保持相同的均匀性属性，就应该可以重现六个月前的研究。如果使用复杂的贝叶斯优化软件，实现可能在版本之间以重要方式发生变化，使得重现旧搜索变得更加困难。并不总是可以回滚到旧实现（例如，如果优化工具作为服务运行）。
    3.  它对搜索空间的均匀探索使得更容易推理结果以及它们可能对搜索空间的建议。
        -   例如，如果准随机搜索遍历中的最佳点在搜索空间的边界上，这是一个很好的（但不是万无一失的）信号，表明应该更改搜索空间边界。[此部分](#识别不良的搜索空间边界)更深入地讨论了这一点。然而，自适应黑盒优化算法可能因为一些不幸的早期试验而忽略了搜索空间的中间，即使它恰好包含同样好的点，因为这正是一个好的优化算法需要采用的非均匀性来加速搜索。
    4.  使用准随机搜索（或其他非自适应搜索算法）时，并行与顺序运行不同数量的试验不会产生统计上不同的结果，这与自适应算法不同。
    5.  更复杂的搜索算法可能并不总是正确处理不可行点，特别是如果它们不是针对神经网络超参数调优而设计的。
    6.  准随机搜索简单，并且在许多调优试验将并行运行时效果特别好。
        -   传闻[^3]，自适应算法很难击败预算为其 2 倍的准随机搜索，特别是当需要并行运行许多试验时（因此在启动新试验时很少有机会利用先前试验的结果）。
        -   如果没有贝叶斯优化和其他高级黑盒优化方法的专业知识，我们可能无法实现它们原则上能够提供的好处。在现实的深度学习调优条件下基准测试高级黑盒优化算法很困难。它们是当前研究的一个非常活跃的领域，更复杂的算法对于缺乏经验的用户来说有其自己的陷阱。这些方法的专家能够获得良好的结果，但在高并行性条件下，搜索空间和预算往往更重要。
-   也就是说，如果我们的计算资源只允许少量试验并行运行，并且我们可以负担按顺序运行许多试验，贝叶斯优化变得更有吸引力，尽管使我们的调优结果更难解释。

[^3]: Ben Recht 和 Kevin Jamieson [指出](http://www.argmin.net/2016/06/20/hypertuning/) 2 倍预算随机搜索作为基线有多强（[Hyperband 论文](https://jmlr.org/papers/volume18/16-558/16-558.pdf)提出了类似的论点），但肯定可以找到搜索空间和问题，其中最先进的贝叶斯优化技术击败了预算为其 2 倍的随机搜索。然而，根据我们的经验，在高并行性状态下击败 2 倍预算随机搜索变得更加困难，因为贝叶斯优化没有机会观察先前试验的结果。

</details>

### 在哪里可以找到准随机搜索的实现？

<details><summary><em>[点击展开]</em></summary>
<br>

-   [Open-Source Vizier](https://github.com/google/vizier) 有一个[准随机搜索的实现](https://github.com/google/vizier/blob/main/vizier/_src/algorithms/designers/quasi_random.py)。在[此使用示例](https://oss-vizier.readthedocs.io/en/latest/guides/user/running_vizier.html)中设置 `algorithm="QUASI_RANDOM_SEARCH"`。
-   另一个替代实现在[这里](https://github.com/mlcommons/algorithmic-efficiency/blob/main/algorithmic_efficiency/halton.py)。
-   上述两种实现都为给定的搜索空间生成 Halton 序列（旨在实现 https://arxiv.org/abs/1706.03200 中推荐的移位、扰乱的 Halton 序列）。
-   如果基于低差异序列的准随机搜索算法不可用，可以用伪随机均匀搜索代替，尽管这可能效率略低。
    -   在 1-2 维中，网格搜索也是可以接受的，尽管在更高维度中不行（参见 [Bergstra & Bengio, 2012](https://www.jmlr.org/papers/v13/bergstra12a.html)）。

</details>

### 使用准随机搜索需要多少次试验才能获得良好结果？

<details><summary><em>[点击展开]</em></summary>
<br>

<p align="center">
<img src="https://raw.githubusercontent.com/google-research/tuning_playbook/main/assets/have_we_sampled_enough.png" width="49%" alt="显示足够采样重要性的箱线图">
</p>

<p align="center"><b>图 3：</b> 在 ImageNet 上使用 100 次试验调优 ResNet-50。通过自举法，模拟了不同数量的调优预算。上面绘制了每个试验预算的最佳性能的箱线图。

-   一般来说，没有办法回答这个问题，但我们可以看看具体的例子。
-   如图 3 所示，研究中的试验次数可能对结果产生重大影响。
    -   注意当采样 6 次试验时，四分位数范围有多大，与采样 20 次试验时相比。
    -   即使有 20 次试验，特别幸运和不幸的研究之间的差异也可能大于使用固定超参数在不同随机种子上重新训练此模型的典型变化，对于此工作负载，验证错误率约为 ~23%，可能在 +/- 0.1% 左右。

</details>

### 如何调试和缓解优化失败？

<details><summary><em>[点击展开]</em></summary>
<br>


***摘要：*** *如果模型遇到优化困难，在尝试其他方法之前修复它们很重要。诊断和纠正训练失败是一个活跃的研究领域。*

<p align="center">
<img src="https://raw.githubusercontent.com/google-research/tuning_playbook/main/assets/stride_instability.png" width="80%" alt="在 WideResnet 中更改单个残差块的步幅会导致训练不稳定。">
</p>


<p align="center"><b>图 4：</b> 在 WideResnet 中更改单个残差块的步幅（2x2 -> 1x1）会导致训练不稳定。这在低学习率下不会降低性能，但由于不稳定性，高学习率不再能够很好地训练。应用 1000 步学习率预热可以解决这种特定的不稳定性实例，允许在最大学习率 0.1 下稳定训练。</p>

#### 识别不稳定的工作负载

-   如果学习率太大，任何工作负载都会变得不稳定。只有当不稳定性迫使我们使用太小的学习率时，不稳定性才是一个问题。
-   至少有两种值得区分的训练不稳定性类型：
    1.  初始化/训练早期的不稳定性。
    2.  训练中期的突然不稳定性。
-   我们可以采用系统方法来识别工作负载中的稳定性问题。
    1.  进行学习率扫描并找到最佳学习率 lr*。
    2.  绘制略高于 lr* 的学习率的训练损失曲线。
    3.  如果学习率 > lr* 显示损失不稳定性（训练期间损失上升而不是下降），那么修复不稳定性很可能会导致更好的训练。
-   在训练期间记录完整损失梯度的 L2 范数，异常值可能导致训练中期的虚假不稳定性。这可以告知如何选择梯度/更新裁剪。

**注意：** 一些模型显示非常早期的不稳定性，然后是导致缓慢但稳定训练的恢复。**常见的评估计划可能会因为评估不够频繁而错过这些问题！**

要检查这一点，我们可以使用 `lr = 2 * 当前最佳值` 进行仅约 ~500 步的缩短运行，但每步都进行评估。

<p align="center">
<img src="https://raw.githubusercontent.com/google-research/tuning_playbook/main/assets/more_frequent_evals.png" width="80%" alt="训练开始时更频繁评估的价值说明。">
</p>

<p align="center"><b>图 5：</b> 训练开始时更频繁评估的价值说明。如果怀疑模型存在早期训练不稳定性，这很有用。</p>

#### 常见不稳定性模式的潜在修复方法

-   应用学习率预热
    -   最适合早期训练不稳定性。
-   应用梯度裁剪
    -   对早期和中期训练不稳定性都有好处，可能修复一些预热无法修复的不良初始化。
-   尝试新的优化器
    -   有时 Adam 可以处理 Momentum 无法处理的不稳定性。这是一个活跃的研究领域。
-   我们可以确保对模型架构使用最佳实践/初始化（下面的示例）。
    -   如果模型尚未包含残差连接和归一化，则添加它们。
-   归一化应该在残差内部。例如 x + f(Norm(x))。
-   Norm(x + f(x)) 已知会导致问题。
-   尝试将残差分支初始化为 0（例如 [ReZero init](https://arxiv.org/abs/2003.04887)）。
-   降低学习率
    -   这是最后的手段。

#### 学习率预热

<p align="center">
<img src="https://raw.githubusercontent.com/google-research/tuning_playbook/main/assets/instability_during_warmup.png" width="80%" alt="预热期间不稳定性的示例（注意水平轴对数刻度）。">
</p>

<p align="center"><b>图 6：</b> 预热期间不稳定性的示例（注意水平轴对数刻度）。在这种情况下，成功训练需要 40k 步的预热。</p>

##### 何时应用学习率预热

<p align="center">
<img src="https://raw.githubusercontent.com/google-research/tuning_playbook/main/assets/axis_model_with_instability.png" width="49%" alt="具有不稳定性的模型的轴图">
</p>

<p align="center"><b>图 7a：</b> 表现出训练不稳定性的模型的超参数轴图示例。最佳学习率处于可行的边缘。"不可行"试验被定义为产生 NaN 或损失的异常高值的试验。</p>

<p align="center">
<img src="https://raw.githubusercontent.com/google-research/tuning_playbook/main/assets/loss_model_with_instability.png" width="49%" alt="具有不稳定性的模型的损失曲线">
</p>

<p align="center"><b>图 7b：</b> 使用我们看到不稳定性的学习率训练的模型的训练损失。</p>

-   图 7a 显示了一个超参数轴图，该图指示模型正在经历优化不稳定性，因为最佳学习率正好在不稳定的边缘。
-   图 7b 显示了如何通过检查使用比此峰值大 5 倍或 10 倍的学习率训练的模型的训练损失来进行双重检查。如果该图显示在稳定下降后损失突然上升（例如在上图中的步骤 ~10k 处），那么模型很可能存在优化不稳定性。

##### 如何应用学习率预热

<p align="center">
<img src="https://raw.githubusercontent.com/google-research/tuning_playbook/main/assets/beneficial_effect_warmup.png" width="80%" alt="预热对训练不稳定性的有益影响">
</p>

<p align="center"><b>图 8：</b> 学习率预热对解决训练不稳定性的有益影响。</p>

-   使用上面的部分，我们假设实践者已经确定了模型变得不稳定的学习率。这是 `unstable_base_learning_rate`。
-   预热涉及在学习率计划前添加一个将学习率从 0 提升到某个稳定的 `base_learning_rate` 的计划，该学习率至少比 `unstable_base_learning_rate` 大一个数量级。默认情况下将尝试 `unstable_base_learning_rate` 的 10 倍的 `base_learning_rate`。尽管请注意，可以对 `unstable_base_learning_rate` 的 100 倍之类的值再次运行整个过程。具体的计划是：
    -   在 `warmup_steps` 上从 0 提升到 `base_learning_rate`。
    -   以恒定速率训练 `post_warmup_steps`。
-   我们的目标是找到允许我们访问远高于 `unstable_base_learning_rate` 的峰值学习率的最短 `warmup_steps` 数量。
-   因此，对于每个 `base_learning_rate`，我们需要调优 `warmup_steps` 和 `post_warmup_steps`。通常可以将 `post_warmup_steps` 设置为 `2*warmup_steps`。
-   预热可以独立于现有的衰减计划进行调优。`warmup_steps` 应该在几个不同的数量级上扫描。例如，示例研究可以尝试 [10, 10<sup>3</sup>, 10<sup>4</sup>, 10<sup>5</sup>]。最大可行点不应超过 `max_train_steps` 的 10%。
-   一旦建立了在 `base_learning_rate` 下不会使训练崩溃的 `warmup_steps`，它就应该应用于基线模型。本质上，我们将此计划添加到现有计划之前，并使用上面讨论的最优检查点选择来将此实验与基线进行比较。例如，如果我们最初有 10,000 `max_train_steps` 并进行了 1000 步的 `warmup_steps`，则新的训练过程应总共运行 11,000 步。
-   如果稳定训练需要长 `warmup_steps`（>5% 的 `max_train_steps`），则可能需要增加 `max_train_steps` 以考虑这一点。
-   在全部工作负载范围内并没有真正的"典型"值。一些模型只需要 100 步，而其他模型（特别是 transformer）可能需要 40k+。

#### 梯度裁剪

<p align="center">
<img src="https://raw.githubusercontent.com/google-research/tuning_playbook/main/assets/gradient_clipping.png" width="80%" alt="梯度裁剪对早期训练不稳定性的影响">
</p>

<p align="center"><b>图 9：</b> 梯度裁剪纠正早期训练不稳定性的说明。</p>

-   当出现大的或异常的梯度问题时，梯度裁剪最有用。
-   裁剪可以修复早期训练不稳定性（早期大梯度范数）或训练中期不稳定性（训练中期突然的梯度尖峰）。
-   有时，较长的预热期可以纠正裁剪无法纠正的不稳定性：参见[上面的部分](#如何应用学习率预热)。
    -   🤖 预热期间裁剪怎么样？
-   理想的裁剪阈值略高于"典型"梯度范数。
-   以下是如何进行梯度裁剪的示例：
    -   如果梯度 $\left | g \right |$ 的范数大于梯度裁剪阈值 $\lambda$，则执行 ${g}'= \lambda \times \frac{g}{\left | g \right |}$，其中 ${g}'$ 是新梯度。
-   在训练期间记录未裁剪的梯度范数。默认情况下，生成：
    -   梯度范数 vs 步骤的图
    -   在所有步骤上聚合的梯度范数直方图
-   基于梯度范数的第 90 百分位数选择梯度裁剪阈值。
    -   阈值将依赖于工作负载，但 90% 是一个很好的起点。如果它不起作用，可以调整此阈值。
    -   🤖 某种自适应策略怎么样？
-   如果我们尝试梯度裁剪但不稳定性问题仍然存在，我们可以更努力地尝试（即使阈值更小）。
-   极其激进的梯度裁剪本质上是降低学习率的一种奇怪方式。如果我们发现自己使用极其激进的裁剪，我们可能应该直接降低学习率。
-   我们通常认为 >50% 的更新以某种方式被裁剪为"极其激进"。
-   如果我们需要进行极其激进的梯度裁剪来处理我们的不稳定性问题，那么我们不如降低学习率。

</details>

### 为什么你把学习率和其他优化参数称为超参数？它们不是任何先验分布的参数。

<details><summary><em>[点击展开]</em></summary>
<br>

-   确实，术语"超参数"在贝叶斯机器学习中有一个精确的[含义](https://en.wikipedia.org/wiki/Hyperparameter)，将学习率和我们在深度学习中调优的大多数其他参数称为"超参数"是对术语的滥用。
-   我们更愿意对学习率、架构参数以及我们在深度学习中调优的所有其他东西使用术语"元参数"，因为它避免了误用"超参数"一词带来的潜在混淆（在讨论贝叶斯优化时特别容易混淆，因为概率响应面模型有其自己的真实超参数）。
-   不幸的是，尽管可能令人困惑，超参数这个术语在深度学习社区中已经变得极为常见。
-   因此，对于像这样一个面向广泛受众的文档，其中包括许多可能不了解这一技术细节的人，我们选择为该领域的一个混淆来源做出贡献，希望避免另一个混淆来源。
-   也就是说，在发表研究论文时我们可能会做出不同的选择，我们鼓励其他人在大多数情况下使用"元参数"。

</details>

### 为什么不应该调整批次大小以直接改善验证集性能？

<details><summary><em>[点击展开]</em></summary>
<br>

-   在*不改变训练流程的任何其他细节*的情况下更改批次大小通常会影响验证集性能。
-   然而，如果为每个批次大小独立优化训练流程，两个批次大小之间的验证集性能差异通常会消失。
-   与批次大小交互最强的超参数，因此对于每个批次大小单独调优最重要的是优化器超参数（例如学习率、动量）和正则化超参数。
    - 由于样本方差，较小的批次大小会在训练算法中引入更多噪声，这种噪声可能具有正则化效果。因此，较大的批次大小可能更容易过拟合，并且可能需要更强的正则化和/或额外的正则化技术。
- 此外，在更改批次大小时，[可能需要调整训练步数](#选择批次大小以最小化训练时间)。
-   一旦考虑了所有这些影响，目前没有令人信服的证据表明批次大小会影响最大可实现的验证性能（参见 [Shallue et al. 2018](https://arxiv.org/abs/1811.03600)）。

</details>

### 所有流行优化算法的更新规则是什么？

<details><summary><em>[点击展开]</em></summary>

<br>

#### Stochastic gradient descent (SGD)

$$\theta_{t+1} = \theta_{t} - \eta_t \nabla \mathcal{l}(\theta_t)$$

#### Momentum

$$v_0 = 0$$

$$v_{t+1} = \gamma v_{t} + \nabla \mathcal{l}(\theta_t)$$

$$\theta_{t+1} = \theta_{t} - \eta_t v_{t+1}$$

#### Nesterov

$$v_0 = 0$$

$$v_{t+1} = \gamma v_{t} + \nabla \mathcal{l}(\theta_t)$$

$$\theta_{t+1} = \theta_{t} - \eta_t( \gamma v_{t+1} + \nabla \mathcal{l}(\theta_{t}))$$

#### RMSProp

$$v_0 = 1 \text{,} m_0 = 0$$

$$v_{t+1} = \rho v_{t} + (1 - \rho) \nabla \mathcal{l}(\theta_t)^2$$

$$m_{t+1} = \gamma m_{t} + \frac{\eta_t}{\sqrt{v_{t+1} + \epsilon}}\nabla \mathcal{l}(\theta_t)$$

$$\theta_{t+1} = \theta_{t} - m_{t+1}$$

#### ADAM

$$m_0 = 0 \text{,} v_0 = 0$$

$$m_{t+1} = \beta_1 m_{t} + (1 - \beta_1) \nabla \mathcal{l} (\theta_t)$$

$$v_{t+1} = \beta_2 v_{t} + (1 - \beta_2) \nabla \mathcal{l}(\theta_t)^2$$

$$b_{t+1} = \frac{\sqrt{1 - \beta_2^{t+1}}}{1 - \beta_1^{t+1}}$$

$$\theta_{t+1} = \theta_{t} - \alpha_t \frac{m_{t+1}}{\sqrt{v_{t+1}} + \epsilon} b_{t+1}$$

#### NADAM

$$m_0 = 0 \text{,} v_0 = 0$$

$$m_{t+1} = \beta_1 m_{t} + (1 - \beta_1) \nabla \mathcal{l} (\theta_t)$$

$$v_{t+1} = \beta_2 v_{t} + (1 - \beta_2) \nabla \mathcal{l} (\theta_t)^2$$

$$b_{t+1} = \frac{\sqrt{1 - \beta_2^{t+1}}}{1 - \beta_1^{t+1}}$$

$$\theta_{t+1} = \theta_{t} - \alpha_t \frac{\beta_1 m_{t+1} + (1 - \beta_1) \nabla \mathcal{l} (\theta_t)}{\sqrt{v_{t+1}} + \epsilon} b_{t+1}$$

</details>

## 致谢

-   我们非常感谢 Max Bileschi、Roy Frostig、Zelda Mariet、Stan Bileschi、Mohammad Norouzi、Chris DuBois 和 Charles Sutton 阅读手稿并提供宝贵的反馈。
-   我们重用了一些实验数据用于几个图表，这些数据最初是由 Naman Agarwal 为其他联合研究生产的。
-   我们要感谢 Will Chen 在文档呈现方面提供的宝贵建议。
-   我们还要感谢 Rohan Anil 进行有益的讨论。

## 引用

```
@misc{tuningplaybookgithub,
  author = {Varun Godbole and George E. Dahl and Justin Gilmer and Christopher J. Shallue and Zachary Nado},
  title = {Deep Learning Tuning Playbook},
  url = {http://github.com/google-research/tuning_playbook},
  year = {2023},
  note = {Version 1.0}
}
```

## 贡献

-   这不是 Google 官方支持的产品。

-   我们很乐意听到您的反馈！

    -   如果您喜欢这本手册，请[点个星标](https://docs.github.com/en/get-started/exploring-projects-on-github/saving-repositories-with-stars#starring-a-repository)！或者发送电子邮件至 deep-learning-tuning-playbook \[at\] googlegroups.com。推荐信帮助我们证明创建更多这样的资源是合理的。
    -   如果有任何地方看起来不正确，请提交一个issue开始讨论。对于不适合提issue的问题或其他消息，请在 GitHub 上开始一个新的讨论主题。

-   如前言中所述，这是一个活跃的文档。我们预计会进行周期性的改进，既有小的也有大的。如果您想收到通知，请关注我们的仓库（参见[说明](https://docs.github.com/en/account-and-profile/managing-subscriptions-and-notifications-on-github/setting-up-notifications/configuring-notifications#configuring-your-watch-settings-for-an-individual-repository)）。

-   请不要在没有首先通过问题跟踪系统与作者协调的情况下提交拉取请求。

### 贡献者许可协议

对本项目的贡献必须伴随贡献者许可协议（CLA）。您（或您的雇主）保留对您贡献的版权；这只是赋予我们使用和重新分发您的贡献作为项目一部分的权限。请访问 <https://cla.developers.google.com/> 查看您当前存档的协议或签署新协议。

您通常只需要提交一次 CLA，所以如果您已经提交过一次（即使是为了不同的项目），您可能不需要再次提交。

### 代码审查

所有提交，包括项目成员的提交，都需要审查。我们为此使用 GitHub 拉取请求。有关使用拉取请求的更多信息，请参阅 [GitHub 帮助](https://help.github.com/articles/about-pull-requests/)。

### 社区指南

本项目遵循 [Google 的开源社区指南](https://opensource.google/conduct/)。
