import arrow
import torch
import numpy as np
import platform
from torch import save, load
from lightning import Trainer
from torch.utils.data import DataLoader, Subset

from src.plot.sst import plot_sst, plot_sst_diff, plot_2d_kernel_density, plot_nino

from src.config.area import Area
from src.utils.mio import DatasetParams, ModelParams, TrainOutput, write_m

class BaseTrainer:
    """
    è®­ç»ƒå™¨åŸºç±» - é›†æˆæ€§èƒ½ä¼˜åŒ–
    
    å‚æ•°:
        title: str, æ¨¡å‹åç§°
        uid: str, è®­ç»ƒå™¨å”¯ä¸€æ ‡è¯†
        area: Area, åŒºåŸŸ
        model_class: LightningModule, æ¨¡å‹ç±»
        save_path: str, ä¿å­˜è·¯å¾„
        pre_model: LightningModule, é¢„è®­ç»ƒæ¨¡å‹
        dataset_params: dict, æ•°æ®é›†å‚æ•°
        trainer_params: dict, è®­ç»ƒå‚æ•°
        model_params: dict, æ¨¡å‹å‚æ•°
        
    dataset_params:
        seq_len: int, åºåˆ—é•¿åº¦
        offset: int, åç§»é‡
        resolution: float, åˆ†è¾¨ç‡
        
    trainer_params:
        epochs: int, è®­ç»ƒè½®æ•°
        batch_size: int, æ‰¹é‡å¤§å°
        split_ratio: list, è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„åˆ†å‰²æ¯”ä¾‹
        
        # æ€§èƒ½ä¼˜åŒ–å‚æ•°
        num_workers: int, æ•°æ®åŠ è½½å·¥ä½œè¿›ç¨‹æ•° (é»˜è®¤: 8, æ¨èCPUæ ¸å¿ƒæ•°/2)
        pin_memory: bool, æ˜¯å¦å›ºå®šGPUå†…å­˜ (é»˜è®¤: True)
        persistent_workers: bool, æ˜¯å¦ä¿æŒå·¥ä½œè¿›ç¨‹ (é»˜è®¤: True)
        prefetch_factor: int, é¢„å–å› å­ (é»˜è®¤: 2)
        precision: str, è®­ç»ƒç²¾åº¦ (é»˜è®¤: "16-mixed", å¯é€‰: "32", "bf16-mixed")
        accumulate_grad_batches: int, æ¢¯åº¦ç´¯ç§¯æ­¥æ•° (é»˜è®¤: 1)
        gradient_clip_val: float, æ¢¯åº¦è£å‰ªå€¼ (é»˜è®¤: None)
        compile_model: bool, æ˜¯å¦ç¼–è¯‘æ¨¡å‹-PyTorch2.0+ (é»˜è®¤: False)
        compile_mode: str, ç¼–è¯‘æ¨¡å¼ (é»˜è®¤: "default")

    """
    
    def __init__(self,
                 title: str,
                 uid: str,
                 area: Area,
                 model_class = None,
                 dataset_class = None,
                 save_path: str = None,
                 pre_model: bool = False,
                 dataset_params: dict = {},
                 trainer_params: dict = {},
                 model_params: dict = {}):
        
        self.trainer_uid = uid

        self.title = title
        self.area = area
        
        # å·¥å‚ç±»å‹
        self.model_class = model_class
        self.dataset_class = dataset_class

        # å‚æ•°
        self.dataset_params = dataset_params
        self.trainer_params = trainer_params
        self.model_params = model_params
        
        # ä¿å­˜è·¯å¾„å’Œé¢„è®­ç»ƒæ¨¡å‹
        self.save_path = save_path
        self.pre_model = pre_model
        
        # æ¨¡å‹
        if pre_model:
            self.model = load(self.save_path, weights_only=False)
            self.trained = True
        else:
            self.model = None
    
        self.trained = False
    
    def split(self, dataset):
        split_ratio = self.trainer_params.get('split_ratio', [0.8, 0.2])
        batch_size = self.trainer_params.get('batch_size', 20)
        
        # è®¡ç®—è®­ç»ƒé›†å¤§å°ï¼ˆæŒ‰æ—¶é—´é¡ºåºæœ‰åºåˆ†å‰²ï¼‰
        total_size = len(dataset)
        train_size = int(total_size * split_ratio[0])
        
        # æŒ‰æ—¶é—´é¡ºåºåˆ†å‰²ï¼šå‰train_sizeä¸ªæ ·æœ¬ä½œä¸ºè®­ç»ƒé›†ï¼Œåval_sizeä¸ªæ ·æœ¬ä½œä¸ºéªŒè¯é›†
        train_indices = list(range(train_size))
        val_indices = list(range(train_size, total_size))
        
        train_set = Subset(dataset, train_indices)
        val_set = Subset(dataset, val_indices)
        
        # ä¼˜åŒ–çš„DataLoaderé…ç½®
        # Windowsç³»ç»Ÿä¸Šå¤šè¿›ç¨‹DataLoaderå¯èƒ½æœ‰é—®é¢˜ï¼Œé»˜è®¤ä½¿ç”¨å•è¿›ç¨‹
        is_windows = platform.system() == 'Windows'
        default_workers = 0 if is_windows else 8
        
        num_workers = self.trainer_params.get('num_workers', default_workers)
        pin_memory = self.trainer_params.get('pin_memory', True)
        persistent_workers = self.trainer_params.get('persistent_workers', True) and num_workers > 0
        prefetch_factor = self.trainer_params.get('prefetch_factor', 2)
        
        dataloader_kwargs = {
            'batch_size': batch_size,
            'shuffle': False,
            'num_workers': num_workers,
            'pin_memory': pin_memory,
        }
        
        if num_workers > 0:
            dataloader_kwargs['persistent_workers'] = persistent_workers
            if prefetch_factor:
                dataloader_kwargs['prefetch_factor'] = prefetch_factor
        
        train_loader = DataLoader(train_set, **dataloader_kwargs)
        
        # éªŒè¯é›†ä½¿ç”¨è¾ƒå°‘çš„workers
        val_dataloader_kwargs = dataloader_kwargs.copy()
        val_dataloader_kwargs['num_workers'] = max(1, num_workers // 2)
        val_loader = DataLoader(val_set, **val_dataloader_kwargs)
        
        return train_loader, val_loader
        
    def train(self):
        lon = self.area.lon
        lat = self.area.lat
        
        # å¯ç”¨ Tensor Cores ä¼˜åŒ–ï¼ˆé€‚ç”¨äº RTX ç³»åˆ— GPUï¼‰
        if torch.cuda.is_available() and hasattr(torch, 'set_float32_matmul_precision'):
            matmul_precision = self.trainer_params.get('matmul_precision', 'high')
            torch.set_float32_matmul_precision(matmul_precision)
        
        dataset = self.dataset_class(
            lon=lon,
            lat=lat,
            **self.dataset_params
        )
        
        train_loader, val_loader = self.split(dataset)
        
        if not self.pre_model:
            self.model = self.model_class(
                **self.model_params
            )
            
            # PyTorch 2.0+ æ¨¡å‹ç¼–è¯‘
            if self.trainer_params.get('compile_model', False):
                if hasattr(torch, 'compile'):
                    compile_mode = self.trainer_params.get('compile_mode', 'default')
                    print(f"ğŸš€ ç¼–è¯‘æ¨¡å‹ (æ¨¡å¼: {compile_mode})...")
                    self.model = torch.compile(self.model, mode=compile_mode)
                else:
                    print("âš ï¸  PyTorchç‰ˆæœ¬ < 2.0, æ¨¡å‹ç¼–è¯‘ä¸å¯ç”¨")
        
        epochs = self.trainer_params.get('epochs', 100)
        
        # ä¼˜åŒ–çš„Traineré…ç½®
        trainer_config = {
            'max_epochs': epochs,
            'accelerator': 'gpu',
            'enable_checkpointing': False,
            'num_sanity_val_steps': 0,
            'precision': self.trainer_params.get('precision', '16-mixed'),
        }
        
        # æ¢¯åº¦ç´¯ç§¯
        accumulate_grad_batches = self.trainer_params.get('accumulate_grad_batches', 1)
        if accumulate_grad_batches > 1:
            trainer_config['accumulate_grad_batches'] = accumulate_grad_batches
        
        # æ¢¯åº¦è£å‰ª
        gradient_clip_val = self.trainer_params.get('gradient_clip_val', None)
        if gradient_clip_val:
            trainer_config['gradient_clip_val'] = gradient_clip_val
            trainer_config['gradient_clip_algorithm'] = self.trainer_params.get(
                'gradient_clip_algorithm', 'norm'
            )

        trainer = Trainer(**trainer_config)
        
        # æ‰“å°ä¼˜åŒ–é…ç½®æ‘˜è¦
        self._print_optimization_summary(accumulate_grad_batches)
        
        start_time = arrow.Arrow.now().format('YYYY-MM-DD HH:mm:ss')
        print(f"================================================")
        print(f"Model: {self.model_class.__name__} Training Started at: {start_time}")

        import time
        train_start = time.time()
        
        trainer.fit(self.model, train_loader, val_loader)
        
        train_time = time.time() - train_start
        
        end_time = arrow.Arrow.now().format('YYYY-MM-DD HH:mm:ss')
        print(f"Model: {self.model_class.__name__} Training Ended at: {end_time}")
        
        spend_time = arrow.get(end_time) - arrow.get(start_time)
        print(f"Model: {self.model_class.__name__} Training Duration: {spend_time}")
        
        # è®¡ç®—ååé‡
        total_samples = len(train_loader.dataset) * epochs
        throughput = total_samples / train_time if train_time > 0 else 0
        print(f"Training Throughput: {throughput:.2f} samples/second")
        print(f"================================================")

        self.trained = True
        self.output()

        if self.save_path:
            self.save()

        return self.model
    
    def _print_optimization_summary(self, accumulate_grad_batches):
        """æ‰“å°ä¼˜åŒ–é…ç½®æ‘˜è¦"""
        is_windows = platform.system() == 'Windows'
        
        print("\n" + "="*60)
        print("ğŸš€ è®­ç»ƒä¼˜åŒ–é…ç½®")
        print("="*60)
        
        # ç³»ç»Ÿä¿¡æ¯
        if is_windows:
            print(f"\nğŸ’» ç³»ç»Ÿ: Windows (å¤šè¿›ç¨‹æ•°æ®åŠ è½½å·²ç¦ç”¨)")
        
        # æ•°æ®åŠ è½½ä¼˜åŒ–
        print("\nğŸ“¦ æ•°æ®åŠ è½½:")
        num_workers = self.trainer_params.get('num_workers', 0 if is_windows else 8)
        print(f"  â€¢ num_workers: {num_workers}")
        if is_windows and num_workers == 0:
            print(f"    âš ï¸  Windowsç³»ç»Ÿé»˜è®¤ç¦ç”¨å¤šè¿›ç¨‹ï¼Œé¿å…å…¼å®¹æ€§é—®é¢˜")
        print(f"  â€¢ pin_memory: {self.trainer_params.get('pin_memory', True)}")
        print(f"  â€¢ persistent_workers: {self.trainer_params.get('persistent_workers', True) and num_workers > 0}")
        print(f"  â€¢ prefetch_factor: {self.trainer_params.get('prefetch_factor', 2 if num_workers > 0 else 'N/A')}")
        
        # è®­ç»ƒä¼˜åŒ–
        print("\nâš¡ è®­ç»ƒé…ç½®:")
        precision = self.trainer_params.get('precision', '16-mixed')
        print(f"  â€¢ precision: {precision}")
        if precision == '16-mixed':
            print(f"    âœ… æ··åˆç²¾åº¦è®­ç»ƒå·²å¯ç”¨ (FP16+FP32)")
        
        if torch.cuda.is_available():
            matmul_precision = self.trainer_params.get('matmul_precision', 'high')
            print(f"  â€¢ tensor_cores: {matmul_precision} precision")
            print(f"    âœ… Tensor Cores ä¼˜åŒ–å·²å¯ç”¨ (RTX GPU)")
        
        print(f"  â€¢ batch_size: {self.trainer_params.get('batch_size', 20)}")
        
        if accumulate_grad_batches > 1:
            effective_bs = self.trainer_params.get('batch_size', 20) * accumulate_grad_batches
            print(f"  â€¢ gradient_accumulation: {accumulate_grad_batches} (æœ‰æ•ˆbatch_size: {effective_bs})")
        
        if self.trainer_params.get('gradient_clip_val'):
            print(f"  â€¢ gradient_clipping: {self.trainer_params.get('gradient_clip_val')}")
        
        # æ¨¡å‹ç¼–è¯‘
        if self.trainer_params.get('compile_model', False):
            print(f"\nğŸ”§ æ¨¡å‹ç¼–è¯‘:")
            print(f"  â€¢ å·²å¯ç”¨: {self.trainer_params.get('compile_mode', 'default')} æ¨¡å¼")
        
        print("="*60 + "\n")

    def predict(self, offset: int, plot: bool = False) -> tuple:
        
        """
        é¢„æµ‹
        
        :param offset: æ•°æ®åç§»é‡
        :return: è¾“å…¥å’Œé¢„æµ‹è¾“å‡º
        """

        print(self.save_path)

        if not self.trained:
            self.model = load(self.save_path, weights_only=False)
            self.trained = True
            
        if not self.model:
            raise ValueError('æ— å·²è®­ç»ƒæ¨¡å‹')
        
        dataset_params = {
            **self.dataset_params,
            'offset': offset,
        }
        
        pred_dataset = self.dataset_class(
            lon=self.area.lon,
            lat=self.area.lat,
            **dataset_params
        )
        
        pred_loader = DataLoader(pred_dataset, batch_size=1, shuffle=False)
        
        input, output = next(iter(pred_loader))
        ssta = pred_dataset.read_ssta(offset)
        
        pred_output = self.model(input)
        
        input = input.detach().numpy()
        output = output.detach().numpy()
        pred_output = pred_output.detach().numpy()
        
        input = input[0, 0, 0, :, :]
        output = output[0, 0, :, :]
        pred_output = pred_output[0, 0, :, :]
        
        masked = np.isnan(output)
        pred_output[masked] = np.nan
        
        pred_diff = pred_output - output
        
        rmse = np.sqrt(np.nanmean((pred_diff) ** 2))
        r2 = 1 - np.nanmean((pred_diff) ** 2) / np.nanmean((output - np.nanmean(output)) ** 2)
        
        print(f"--------------------------------")
        
        print(f"Model: {self.model_class.__name__} Prediction RMSE: {rmse}")
        
        if plot:
            resolution = self.dataset_params.get('resolution', 1)
            plot_nino(ssta, step=resolution)
            plot_sst(pred_output, self.area.lon, self.area.lat, step=resolution)
            plot_sst_diff(pred_diff, self.area.lon, self.area.lat, step=resolution)
            # ä½¿ç”¨å¢å¼ºç‰ˆæµ·è¡¨æ¸©åº¦åˆ†æï¼Œä¼ é€’äºŒç»´æµ·è¡¨æ¸©åº¦æ•°æ®
            plot_2d_kernel_density(pred_output, self.area.lon, self.area.lat)
            
        return input, output, pred_output, rmse, r2, ssta
    

    def save(self):
        save(self.model, self.save_path)

    def output(self):
        
        model_params = ModelParams(
            model=self.model_class.__name__,
            m_type=self.model_class.__name__,
            model_path=self.save_path,
            params=self.model_params,
        )
        
        offset = self.dataset_params.get('offset', 0)
        
        dataset_params = DatasetParams(
            dataset=self.dataset_class.__name__,
            range=[self.area.lon, self.area.lat],
            resolution=self.dataset_params.get('resolution', 1),
            start_time=arrow.get(2004, 1, 1).shift(months=offset).format('YYYY-MM-DD'),
            end_time=arrow.get(2024, 12, 31).format('YYYY-MM-DD'),
        )
        
        train_output = TrainOutput(
            epoch=self.trainer_params.get('epochs', 100),
            val_loss=self.model.val_loss if hasattr(self.model, 'val_loss') else [],
            batch_size=self.trainer_params.get('batch_size', 20),
            train_loss=self.model.train_loss if hasattr(self.model, 'train_loss') else [],
            m_params=model_params,
            d_params=dataset_params,
        )
        
        write_m(train_output, self.title, self.trainer_uid)
